{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ccb4b6-05e4-417d-8ebe-5477d16550b8",
   "metadata": {},
   "source": [
    "# PyTorch：view() 与 reshape() 区别详解\n",
    "<font size=4 color=red>\n",
    "    总之，两者都是用来重塑tensor的shape的。view只适合对满足连续性条件（contiguous）的tensor进行操作，而reshape同时还可以对不满足连续性条件的tensor进行操作，具有更好的鲁棒性。view能干的reshape都能干，如果view不能干就可以用reshape来处理。别看目录挺多，但内容很细呀~其实原理并不难啦~我们开始吧~ \n",
    "</font></br>\n",
    "\n",
    "\n",
    "\n",
    "## 主要内容\n",
    "\n",
    "### 一、PyTorch中tensor的存储方式\n",
    "\n",
    "    1、PyTorch张量存储的底层原理\n",
    "\n",
    "    2、PyTorch张量的步长（stride）属性\n",
    "\n",
    "### 二、对“视图(view)”字眼的理解\n",
    "\n",
    "### 三、view() 和reshape() 的比较\n",
    "\n",
    "    1、对 torch.Tensor.view() 的理解\n",
    "\n",
    "    2、对 torch.reshape() 的理解\n",
    "\n",
    "### 四、总结\n",
    "\n",
    "### 一、PyTorch中tensor的存储方式\n",
    "<font size=4>想要深入理解view与reshape的区别，首先要理解一些有关PyTorch张量存储的底层原理，比如tensor的头信息区（Tensor）和存储区 （Storage）以及tensor的步长Stride。不用慌，这部分的原理其实很简单的(^-^)!</font>\n",
    "\n",
    "<font size=5>1、PyTorch张量存储的底层原理</font></br>\n",
    "<font size=4>tensor数据采用头信息区（Tensor）和存储区 （Storage）分开存储的形式，如图所示。变量名以及其存储的数据是分为两个区域分别存储的。比如，我们定义并初始化一个tensor，tensor名为A，A的形状size、步长stride、数据的索引等信息都存储在头信息区（Tensor），而A所存储的真实数据则存储在存储区（Storage）。另外，如果我们对A进行截取、转置或修改等操作后赋值给B，则B的数据共享A的存储区（Storage），存储区（Storage）的数据数量没变，变化的只是B的头信息区（Tensor）对数据的索引方式。</font>\n",
    "</br>\n",
    "<img src=\"https://github.com/jhong-tao/s2dl/blob/master/ipynb/0-imgs/217856618_1_20210317091321662.jpg?raw=true\" width=500 >\n",
    "</br>（Tensor）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e96e2-ef2f-4eba-8e7e-a5264373d072",
   "metadata": {},
   "source": [
    "## 共用存储区域举例\n",
    "<font size=4>共用存储区域，当一个变量修改存储区的数据时，另一个变量的值也会跟着变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b84503f-3c82-49b9-b6b2-9496ba65e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([0, 1, 2, 3, 4])\n",
      "b: tensor([2, 3, 4])\n",
      "id of storage of a: 2175559017080\n",
      "id of storage of b: 2175559017080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(5)  # 初始化张量 a 为 [0, 1, 2, 3, 4]\n",
    "b = a[2:]            # 截取张量a的部分值并赋值给b，b其实只是改变了a对数据的索引方式\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('id of storage of a:', id(a.storage))  # 打印a的存储区地址\n",
    "print('id of storage of b:', id(b.storage))  # 打印b的存储区地址,可以发现两者是共用存储区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291a3cf2-de84-493e-94f2-b6aa30737b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([0, 1, 2, 0, 4])\n",
      "b: tensor([2, 0, 4])\n",
      "id of storage of a: 2175478696680\n",
      "id of storage of b: 2175478696680\n"
     ]
    }
   ],
   "source": [
    "b[1] = 0    # 修改b中索引为1，即a中索引为3的数据为0\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('id of storage of a:', id(a.storage))  # 打印a的存储区地址,可以发现a的相应位置的值也跟着改变，说明两者是共用存储区\n",
    "print('id of storage of b:', id(b.storage))  # 打印b的存储区地址"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50296a-ba54-48c4-8b5b-6e71fc5bf114",
   "metadata": {},
   "source": [
    "### PyTorch张量的步长（stride）属性\n",
    "<font size=4> \n",
    "torch的tensor也是有步长（stride）属性的，说起stride属性是不是很耳熟？是的，卷积神经网络中卷积核对特征图的卷积操作也是有stride属性的，但这两个stride可完全不是一个意思哦。\n",
    "</font>\n",
    "<br>\n",
    "<font size=4 color=red>\n",
    "stride是在指定维度dim中从一个元素跳到下一个元素所必需的步长。当没有参数传入时，返回所有步长的元组。否则，将返回一个整数值作为特定维度dim中的步长。<br>\n",
    "tensor的步长可以理解为从指定的维度中的一个元素到下一个元素中间的跨度。<br>为方便理解，就直接用图说明了，您细细品(^-^)：    \n",
    "</font>\n",
    "<img src=\"\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-ml(python3.7)",
   "language": "python",
   "name": "study-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
