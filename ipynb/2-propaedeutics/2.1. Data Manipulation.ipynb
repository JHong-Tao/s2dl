{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09cfd53-5765-4dc2-99f8-0b9afbe53717",
   "metadata": {},
   "source": [
    "# 2.1. 数据操作\n",
    "## 2.1.1. 入门\n",
    "- <font size=4> torch.arange()创建一维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6c2ca50-2002-4cc3-b5dc-8cf5440118df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"  # 配置单元格支持多个输出\n",
    "x = torch.arange(12)  # torch.arange()方法创建长度为12的一维数组，从0到11\n",
    "# print(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537a1b9-930c-4b40-a01a-a53558e7fbfb",
   "metadata": {},
   "source": [
    "- <font size=4>通过张量的shape属性来查看张量的维度和大小，也就是查看张量（沿每个轴的长度）的形状 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a561c8-b82f-4318-890f-36b40deea70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa44dac-b0c1-4589-b96c-28760a8746d8",
   "metadata": {},
   "source": [
    "- <font size=4>可以通过张量的size()方法来查看张量的形状和大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57184217-1770-412b-8b01-931aa203833b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()  # 属性和方法的区别，属性不需要括号，方法需要加上括号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d9e57-71b9-4ac4-9288-bf8298832305",
   "metadata": {},
   "source": [
    "- <font size=4>可以通过张量的numel()方法来查看张量的元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dcd0c31-b3f0-4f1e-b521-20eab6c9603e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd66fea8-ed32-46d5-8730-9a9e3ca7bfc0",
   "metadata": {},
   "source": [
    "- <font size=4>可以通过张量的reshape()函数来改变张量的形状，但是这个过程不会改变张量的元素的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dba6183-1368-40d5-8f98-98041f2cbf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n",
    "x1 = x.reshape(3, 4)  # 将x是1行12列，修改为3行4列\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b143ffb-6bec-432c-9d21-cc81b5821f3a",
   "metadata": {},
   "source": [
    "- <font size=4>张量的reshape()方法还能自动推理改变张量维度大小的一个维度，用“-1”代替该维度的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6635059a-eaac-43af-b0be-d14c2c70a89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1,2)  # 比如该列中，只给了将1行12列的张量x转为2列6行的张量，只要显性的给出任意一个维度的值，另一维度用-1代替，reshape()方法会自动推断另一维度的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8772fe49-f727-4e95-928b-0f104b02330e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5]],\n",
       "\n",
       "        [[ 6,  7,  8],\n",
       "         [ 9, 10, 11]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(2,2,-1)  # 该例中，自动推断了2个2行3列的张量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08db23d-c411-4788-8dac-6730309c22ef",
   "metadata": {},
   "source": [
    "- <font size=4> 也可以通过张量的view()函数来改变张量的视图形状，view()函数与reshape()的区别是，view()只能针对tensor的storage存储是连续的情况，reshape()则没有限制，reshape=contiguous+view</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e32be1e4-f819-4294-a4e3-db55f9e9e1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(4, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb1e75-0a87-4dd3-b74e-d9f2605f79fa",
   "metadata": {},
   "source": [
    "- <font size=4>\n",
    "    同样，我们可以创建一个形状为(2,3,4)的张量，其中所有元素都设置为1。代码如下：\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e2995d1-156c-46eb-8c08-c5c39ce2276d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfdf5e-13d8-4946-9683-4df975b59351",
   "metadata": {},
   "source": [
    "- <font size=4>\n",
    "    有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。 \n",
    "    </font>\n",
    "    <font size=4 color=red>\n",
    "    其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d38eec41-3c06-4ac5-964e-3514a928a371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8719,  0.6763, -1.1493, -1.1393],\n",
       "        [-0.5862,  0.0528,  0.1616,  0.5389],\n",
       "        [ 1.3259,  2.1116, -0.2726,  0.3273]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d1b63-7a25-4e46-b830-ff3cd4ef9287",
   "metadata": {},
   "source": [
    "- <font size=4>\n",
    "    我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 \n",
    "    </font>\n",
    "    <font size=4 color=red>\n",
    "    在这里，最外层的列表对应于轴0，内层的列表对应于轴1。\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "274d8377-6273-4d8b-b86f-d28c62040d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8689e-2ceb-4c80-b6c6-83cfacace3f6",
   "metadata": {},
   "source": [
    "## 2.1.2. 运算符\n",
    "<font size=4>\n",
    "    我们的兴趣不仅限于读取数据和写入数据。 我们想在这些数据上执行数学运算，其中最简单且最有用的操作是按元素（elementwise）运算。 它们将标准标量运算符应用于数组的每个元素。 对于将两个数组作为输入的函数，按元素运算将二元运算符应用于两个数组中的每对位置对应的元素。 我们可以基于任何从标量到标量的函数来创建按元素函数。\n",
    "    </font><br><br>\n",
    "    <font size=4>\n",
    "    在数学表示法中，我们将通过符号 $f:R→R$  来表示一元标量运算符（只接收一个输入）。 这意味着该函数从任何实数（ $R$ ）映射到另一个实数。 同样，我们通过符号 $f:R,R→R$  表示二元标量运算符，这意味着该函数接收两个输入，并产生一个输出。 给定同一形状的任意两个向量 $u$ 和 $v$ 和二元运算符 $f$ ， 我们可以得到向量 $c=F(u,v)$ 。 具体计算方法是 $c_{i}←f(u_{i},v_{i})$ ， 其中 $c_{i} 、 u_{i}$ 和 $v_{i}$ 分别是向量 $c 、 u$ 和 $v$ 中的元素。 在这里，我们通过将标量函数升级为按元素向量运算来生成向量值  $F:Rd,Rd→Rd$ 。\n",
    "    </font><br><br>\n",
    "    <font size=4>\n",
    "    对于任意具有相同形状的张量， 常见的标准算术运算符$（+、-、*、/和**）$都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。 在下面的例子中，我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。\n",
    "    </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c25d8a26-ae3f-49fd-b16b-18764f8f82ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  4,  6, 10])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1,  0,  2,  6])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  8, 16])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 1.0000, 2.0000, 4.0000])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4, 16, 64])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x+y\n",
    "x-y\n",
    "x*y\n",
    "x/y\n",
    "x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64b5fb-99e3-4908-9663-c6bf89f679ad",
   "metadata": {},
   "source": [
    "- <font size=4>\n",
    "    按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c52bdff-94da-436d-b63f-d3577c4b3129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)  # 以e为底x的多少次幂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51499003-128f-41eb-a59c-a5d351efbd4f",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    除了按元素计算外，我们还可以执行线性代数运算，包括向量点积和矩阵乘法。 <br><br>\n",
    "\n",
    "我们也可以把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。 下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。 我们可以看到，第一个输出张量的轴-0长度（ 6 ）是两个输入张量轴-0长度的总和（ 3+3 ）； 第二个输出张量的轴-1长度（ 8 ）是两个输入张量轴-1长度的总和（ 4+4 ）。\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2a692821-8328-49d8-9470-b8c73b895d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 4., 3.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [4., 3., 2., 1.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "x\n",
    "y\n",
    "torch.cat((x, y), dim=0)  # 将x与y沿着第0维拼接，也就是按行拼接\n",
    "torch.cat((x, y), dim=1)  # 将x与y沿着第1维拼接，也就是按列拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef9161-6a62-49d9-928a-4565e1079229",
   "metadata": {},
   "source": [
    "- <font size=4>\n",
    "    有时，我们想通过逻辑运算符构建二元张量。 以X == Y为例： 对于每个位置，如果X和Y在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句X == Y在该位置处为真，否则该位置为0。\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c404264-006e-488a-9e83-b47e79db358c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == y  # 判断x与y对应位置的元素值是否相等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f259c-993a-43e1-84f7-9b86633aabb2",
   "metadata": {},
   "source": [
    "- <font size=4>\n",
    "    对张量中的所有元素进行求和，会产生一个单元素张量。\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b52ea8e-ce16-4750-adca-2eb07a3f3916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()  # 累加tensor 中的所有元素的和"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e45ef9-c903-4dbd-b516-280062b968a5",
   "metadata": {},
   "source": [
    "## 2.1.3. 广播机制\n",
    "<font size=4>\n",
    "    在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：<font color=red>首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。</font><br><br>\n",
    "\n",
    "在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4355af79-af67-42b1-938a-0bc3c9c93178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape(3, 1)\n",
    "b = torch.arange(2).reshape(1, 2)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060ee76-0e4f-4edd-8c14-1b9d9bad0fbd",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "    由于a和b分别是 $3×1$ 和 $1×2$ 矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的 $3×2$ 矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。\n",
    "    <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "91716766-8cb8-4794-ac9e-73d30a2bacb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba67bf-8f57-4589-b76b-71fca6f81897",
   "metadata": {},
   "source": [
    "## 2.1.4. 索引和切片\n",
    "<font size=4>\n",
    "    就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。<br><br>\n",
    "\n",
    "如下所示，我们可以用[-1]选择最后一个元素，可以用[1:3]选择第二个和第三个元素：\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8bbc68e9-97aa-48d4-bbdd-9894aa77ac2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 4., 8.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n",
    "x[-1]\n",
    "x[1:3]\n",
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224f2db-5493-427b-8419-8739c50d1c8b",
   "metadata": {},
   "source": [
    "- <font size=4>\n",
    "    除读取外，我们还可以通过指定索引来将元素写入矩阵。\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4862433f-0cc2-45ff-935f-72df02f0438f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1, 2] = 9  # 修改tensor x 的第2行第三列为9\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8022c1-23d8-4c61-a9dc-0f3df7e68a2d",
   "metadata": {},
   "source": [
    "## 2.1.5. 节省内存\n",
    "<font size=4>\n",
    "    运行一些操作可能会导致为新结果分配内存。 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。<br><br>\n",
    "\n",
    "在下面的例子中，我们用Python的id()函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。 这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1288e2c5-8e58-4442-90d2-f7ff78c1861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1346187134840"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1346118481280"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1346187164072"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1346118473728"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(y)  # y的地址\n",
    "y.storage().data_ptr()  # y的数据的起始地址\n",
    "y = x+y\n",
    "id(y)\n",
    "y.storage().data_ptr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99f084-c75a-47ae-9e4a-62317e9d6cd7",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。 在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。 其次，如果我们不原地更新，其他引用仍然会指向旧的内存位置， 这样我们的某些代码可能会无意中引用旧的参数。<br><br>\n",
    "    幸运的是，执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，<font color=red>例如Y[:] =expression。 为了说明这一点，我们首先创建一个新的矩阵Z，其形状与另一个Y相同， 使用zeros_like来分配一个全 0 的块。</font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "60e8a8d2-ecd6-41e7-b93f-9f88b5cf07cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1346186657864"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1346186657864"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.zeros_like(y)\n",
    "id(z)  # z之前的地址\n",
    "z[:] = x+y\n",
    "id(z)  # z之后的地址，可见此时z并没有被赋予新的地址，也就是并没有开辟新的内从空间，只是覆盖掉了之前的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff63d0-dde0-4a4e-914d-f912f1e94a93",
   "metadata": {},
   "source": [
    "- <font size=4>\n",
    "    如果在后续计算中没有重复使用X， 我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。\n",
    "    <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "43b0af2e-3087-4463-b37b-9338bf1624de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1346187502200"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1346187502200"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(x)  # x 之前的地址\n",
    "x += y # += 计算\n",
    "id(x)  # x之后的地址，可见执行+=操作和地址不变，不会开辟新的内存空间，而x = x+y会开辟新的内存空间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fd58a-7245-470f-9f1b-7f9c82aa0969",
   "metadata": {},
   "source": [
    "## 2.1.6. 转换为其他Python对象\n",
    "<font size=4>\n",
    "    将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e6308a5f-eb0f-45b4-a44f-8ea7ef5785e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.5000])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a\n",
    "a.item()  # 将tensor转换为python数据类型\n",
    "type(a.item())\n",
    "int(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-ml(python3.7)",
   "language": "python",
   "name": "study-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
