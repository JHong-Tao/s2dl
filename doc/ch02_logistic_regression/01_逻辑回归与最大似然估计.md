#  最大似然估计与逻辑回归

# 1. 最大似然估计MLE(Maximum Likelihood Estimation)

## 1.1 相关概念

### 1.1.1 先验概率

**先验概率是基于背景常识或者历史数据的**统计**得出的**预判概率，一般只包含一个变量，例如：$P(X)$

### 1.1.2 条件概率

**条件概率是表示一个事件发生后另一个事件发生的概率**，例如$P(Y|X)$代表$X$事件发生后$Y$事件发生的概率。

### 1.1.3 后验概率

后验概率是由果求因，也就是在知道结果的情况下求原因的概率，例如Y事件是X引起的，那么$P(X|Y)$就是后验概率，也可以说它是事件发生后的反向条件概率。



## **1.2  概率（possibility）与似然（likelihood）**

### 1.2.1 概率（possibility）与似然（likelihood）

**概率$P(x|\theta)$：**用于在已知参数$\theta$的情況下关于联合样本值的$x$联合密度函数，用来预测接下来在观测上所得到的结果的可能性p；

**似然$L(\theta|x)$：**用于在已知某些观测所得到的结果$x$时,也就是给定联合样本值$x$，下关于(未知）参数$\theta$的函数（模型已经知道，但是具体的参数未知），用来对有关事物的性质的参数$\theta$进行估值。

也就是说**概率**用于在已知一些参数的情况下，预测接下来的观测所得到的结果；而**似然**则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

$P(x|\theta)$ 是参数$\theta$ 下 $x$ 出现的可能性，同时也是$x$出现时参数为$\theta$的似然。

这个怎么理解呢？很简单。

$P(x|\theta)$ 越大，就说明参数如果为$\theta$，你的观测数据$x$ 就越可能出现。那你既然已经观测到数据 $x$ 了，那么**越可能让这个$x$ 出现的参数 $\theta$，就越是一个靠谱的模型参数。这个参数的靠谱程度，就是似然。**

**所以 $P(x|\theta)$ 是 $\theta$参数下 $x$ 出现的概率，同时也是 $x$ 出现时 $\theta$参数的靠谱程度，也就是$\theta$的似然。**

**而贝叶斯公式告诉我们，靠谱程度，也就是似然，与概率是成正比的。**

当然这里的成正比并不是说你直接归一化似然就得到了概率。因为靠谱程度可能有多个影响因素，这里只是考虑了观测数据 $x$ 对 $θ$ 的靠谱程度的影响。而 $θ$ 本身也可能有自己的先验概率 $P(θ)$，或者有其他观测 $y$ 可能从另一个视角提供了 $θ$ 的靠谱程度 $P(y|θ)$。

但如果你已经收集到了 $θ$ 的所有似然，你就可以把这些似然函数相乘，得到 $θ$ 最终的靠谱程度 ，上面的例子里是 $P(x|θ)P(y|θ)P(θ)$。这个函数直接归一化，就得到了 θ 的概率。

最大似然估计是，当我们**知道总体的概率分布模型，但是不知道概率分布函数中的参数的情况下**，**由样本来估计参数所用的方法**。用一句话来说就是，**当函数的参数是多少的情况下，使我们得到当前已经得到的样本的概率最大。**（多说一句，概率分布模型就是概率密度函数）

**用数学表达式描述：**

给定输出 ![[公式]](https://www.zhihu.com/equation?tex=x+) （这里的 ![[公式]](https://www.zhihu.com/equation?tex=x+) 是指联合样本随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X%3Dx) 取到的值，即 ![[公式]](https://www.zhihu.com/equation?tex=X%3Dx) ），关于参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) (未知)的函数 ![[公式]](https://www.zhihu.com/equation?tex=L%28%CE%B8%7Cx%29) 。它（在数值上）等于给定参数 ![[公式]](https://www.zhihu.com/equation?tex=%CE%B8) 后变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 取某个值的概率：
$$
L(\theta|X=x)=P(X=x|\theta)=f(X=x;\theta)
$$

这个等式表示的是对于事件发生的两种角度的看法。其实等式两遍都是表示的这个事件发生的概率或者说可能性。再给定一个样本$x$后，我们去想这个样本出现的可能性到底是多大。统计学的观点始终是认为样本的出现是基于一个分布的。那么我们去假设这个分布为$p$，里面有参数$\theta$。对于不同的$\theta$，样本的分布不一样。$P(x|\theta)$表示的就是在给定参数$\theta$的情况下，$x$出现的可能性多大。$L(\theta|x)$表示的是在给定样本$x$的时候，哪个参数$\theta$使得$x$出现的可能性为多大。所以其实这个等式要表示的核心意思都是在给一个$\theta$和一个样本$x$的时候，整个事件发生的可能性多大。

### 1.**2.2 概率与似然的联系：**

先看似然函数的定义，它是给定联合样本值![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)下关于(未知)参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的函数：![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta+%7C+%5Ctextbf%7Bx%7D%29+%3D+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29)

这里的小![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)是指联合样本随机变量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)取到的值，即![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D)；

这里的![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)是指未知参数，它属于参数空间；

这里的![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ctextbf%7Bx%7D%7C%5Ctheta%29)是一个密度函数，特别地，它表示(给定)![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)下关于联合样本值![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)的联合密度函数。



所以从定义上，似然函数和密度函数是完全不同的两个**数学对象**：前者是关于![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)的函数，后者是关于![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)的函数。所以这里的等号![[公式]](https://www.zhihu.com/equation?tex=%3D) 理解为**函数值形式**的相等，而不是两个函数本身是同一函数(根据函数相等的定义，函数相等当且仅当定义域相等并且对应关系相等)。

### 1.**2.3 概率与似然的区别**：

（1）如果![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)是离散的随机向量，那么其**概率**密度函数![[公式]](https://www.zhihu.com/equation?tex=+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29)可改写为![[公式]](https://www.zhihu.com/equation?tex=+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29+%3D+%5Cmathbb%7BP%7D_%5Ctheta%28%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D%29)，即代表了在参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)下随机向量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)取到值![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)的**可能性**；并且，**如果**我们发现

![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta_1+%7C+%5Ctextbf%7Bx%7D+%29+%3D+%5Cmathbb%7BP%7D_%7B%5Ctheta_1%7D%28%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D%29+%3E+%5Cmathbb%7BP%7D_%7B%5Ctheta_2%7D%28%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D%29+%3D+L%28%5Ctheta_2+%7C+%5Ctextbf%7Bx%7D%29)

那么**似然**函数就反应出这样一个**朴素推测**：在参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1)下随机向量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)取到值![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)的**可能性大于** 在参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2)下随机向量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)取到值![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)的**可能性**。换句话说，我们更有理由相信(相对于![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2)来说)![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1)

**更有可能**是真实值。这里的可能性由概率来刻画。

（2）如果![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)是连续的随机向量，那么其密度函数![[公式]](https://www.zhihu.com/equation?tex=+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29)本身（如果在![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)连续的话）在![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)处的**概率**为0，为了方便考虑一维情况：给定一个充分小![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon+%3E+0)，那么随机变量![[公式]](https://www.zhihu.com/equation?tex=X)取值在![[公式]](https://www.zhihu.com/equation?tex=%28x+-+%5Cepsilon%2C+x+%2B+%5Cepsilon%29)区间内的**概率**即为

![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BP%7D_%5Ctheta%28x+-+%5Cepsilon+%3C+X+%3C+x+%2B+%5Cepsilon%29+%3D+%5Cint_%7Bx+-+%5Cepsilon%7D%5E%7Bx+%2B+%5Cepsilon%7D+f%28x+%7C+%5Ctheta%29+dx+%5Capprox+2+%5Cepsilon+f%28x+%7C+%5Ctheta%29+%3D+2+%5Cepsilon+L%28%5Ctheta+%7C+x%29)

并且两个未知参数的情况下做比就能约掉![[公式]](https://www.zhihu.com/equation?tex=2%5Cepsilon)，所以和离散情况下的理解一致，只是此时**似然**所表达的那种**可能性**和**概率**![[公式]](https://www.zhihu.com/equation?tex=f%28x%7C%5Ctheta%29+%3D+0)无关。

综上，**概率**(密度)表达给定![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)下样本随机向量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D)的**可能性**，而**似然**表达了给定样本![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D)下参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1)(相对于另外的参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2))为真实值的**可能性**。我们总是对随机变量的取值谈**概率**，而在非贝叶斯统计的角度下，参数是一个实数而非随机变量，所以我们一般不谈一个参数的**概率**。

最后我们再回到![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta+%7C+%5Ctextbf%7Bx%7D%29+%3D+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29)这个表达。首先我们严格记号，竖线![[公式]](https://www.zhihu.com/equation?tex=%7C)表示条件概率或者条件分布，分号![[公式]](https://www.zhihu.com/equation?tex=%3B)表示把参数隔开。所以这个式子的严格书写方式是![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta+%7C+%5Ctextbf%7Bx%7D%29+%3D+f%28%5Ctextbf%7Bx%7D+%3B+%5Ctheta%29)因为![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)在右端只当作参数理解。

（1）如果![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)是离散的随机向量，那么其**概率**密度函数![[公式]](https://www.zhihu.com/equation?tex=+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29)可改写为![[公式]](https://www.zhihu.com/equation?tex=+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29+%3D+%5Cmathbb%7BP%7D_%5Ctheta%28%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D%29)，即代表了在参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)下随机向量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)取到值![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)的**可能性**；并且，**如果**我们发现

![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta_1+%7C+%5Ctextbf%7Bx%7D+%29+%3D+%5Cmathbb%7BP%7D_%7B%5Ctheta_1%7D%28%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D%29+%3E+%5Cmathbb%7BP%7D_%7B%5Ctheta_2%7D%28%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D%29+%3D+L%28%5Ctheta_2+%7C+%5Ctextbf%7Bx%7D%29)

那么**似然**函数就反应出这样一个**朴素推测**：在参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1)下随机向量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)取到值![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)的**可能性大于** 在参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2)下随机向量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)取到值![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)的**可能性**。换句话说，我们更有理由相信(相对于![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2)来说)![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1)

**更有可能**是真实值。这里的可能性由概率来刻画。

（2）如果![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D)是连续的随机向量，那么其密度函数![[公式]](https://www.zhihu.com/equation?tex=+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29)本身（如果在![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)连续的话）在![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D)处的**概率**为0，为了方便考虑一维情况：给定一个充分小![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon+%3E+0)，那么随机变量![[公式]](https://www.zhihu.com/equation?tex=X)取值在![[公式]](https://www.zhihu.com/equation?tex=%28x+-+%5Cepsilon%2C+x+%2B+%5Cepsilon%29)区间内的**概率**即为

![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BP%7D_%5Ctheta%28x+-+%5Cepsilon+%3C+X+%3C+x+%2B+%5Cepsilon%29+%3D+%5Cint_%7Bx+-+%5Cepsilon%7D%5E%7Bx+%2B+%5Cepsilon%7D+f%28x+%7C+%5Ctheta%29+dx+%5Capprox+2+%5Cepsilon+f%28x+%7C+%5Ctheta%29+%3D+2+%5Cepsilon+L%28%5Ctheta+%7C+x%29)

并且两个未知参数的情况下做比就能约掉![[公式]](https://www.zhihu.com/equation?tex=2%5Cepsilon)，所以和离散情况下的理解一致，只是此时**似然**所表达的那种**可能性**和**概率**![[公式]](https://www.zhihu.com/equation?tex=f%28x%7C%5Ctheta%29+%3D+0)无关。

综上，**概率**(密度)表达给定![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)下样本随机向量![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D)的**可能性**，而**似然**表达了给定样本![[公式]](https://www.zhihu.com/equation?tex=%5Ctextbf%7BX%7D+%3D+%5Ctextbf%7Bx%7D)下参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1)(相对于另外的参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2))为真实值的**可能性**。我们总是对随机变量的取值谈**概率**，而在非贝叶斯统计的角度下，参数是一个实数而非随机变量，所以我们一般不谈一个参数的**概率**。

最后我们再回到![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta+%7C+%5Ctextbf%7Bx%7D%29+%3D+f%28%5Ctextbf%7Bx%7D+%7C+%5Ctheta%29)这个表达。首先我们严格记号，竖线![[公式]](https://www.zhihu.com/equation?tex=%7C)表示条件概率或者条件分布，分号![[公式]](https://www.zhihu.com/equation?tex=%3B)表示把参数隔开。所以这个式子的严格书写方式是![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta+%7C+%5Ctextbf%7Bx%7D%29+%3D+f%28%5Ctextbf%7Bx%7D+%3B+%5Ctheta%29)因为![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)在右端只当作参数理解。

## 1.3. 最大似然估计原理

### 1.3.1 最大似然估计法的引例

第一个例子还是盒子摸球的例子。

有两个盒子，一号盒子里面有100个球，其中99个是白球，1个是黑球；二号盒子里面也有100个球，其中99个是黑球，1个是白球。

现在我告诉你，我从其中某一个盒子中随机摸出来一个球，这个球是白球，那么你说，我更有可能是从哪个盒子里摸出的这个球？

显然，你会说是一号盒子。道理很简单，因为一号盒子当中，摸出白球的概率是0.99，而二号盒子摸出白球的概率是0.01。显然更有可能是一号盒子了。

第二个例子也是大家熟悉的丢硬币的例子。

我有三个不均匀的硬币，其中第一个硬币抛出正面的概率是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B2%7D%7B5%7D) ，第二个硬币抛出正面的概率是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D) ，第三个硬币抛出正面的概率是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B3%7D%7B5%7D) ，这时我取其中一个硬币，抛了20次，其中正面向上的次数是13次，请问我最有可能是拿的哪一个硬币？

思考的过程也很简单：

三枚硬币，抛掷20次，13次正面向上的概率分别是：

第一枚： ![[公式]](https://www.zhihu.com/equation?tex=C_%7B20%7D%5E%7B13%7D%28%5Cfrac%7B2%7D%7B5%7D%29%5E%7B13%7D%281-%5Cfrac%7B2%7D%7B5%7D%29%5E%7B20-13%7D%3D0.014563052125736147)

第二枚： ![[公式]](https://www.zhihu.com/equation?tex=C_%7B20%7D%5E%7B13%7D%28%5Cfrac%7B1%7D%7B2%7D%29%5E%7B13%7D%281-%5Cfrac%7B1%7D%7B2%7D%29%5E%7B20-13%7D%3D0.0739288330078125)

第三枚： ![[公式]](https://www.zhihu.com/equation?tex=C_%7B20%7D%5E%7B13%7D%28%5Cfrac%7B3%7D%7B5%7D%29%5E%7B13%7D%281-%5Cfrac%7B3%7D%7B5%7D%29%5E%7B20-13%7D%3D0.1658822656197132)



### 1.3.2 似然函数的由来

有了这个例子，下面我们开始介绍最大似然估计方法。我们重点要理解的是**似然**这个词，这个词听起来比较陌生。

我们首先看离散型的情形，随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 的概率分布已知，但是这个分布的参数是未知的，需要我们去估计，我们把他记作是 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) ，好比上面抛掷硬币的试验中，硬币正面朝上的概率是未知的，需要我们去估计，那么此时 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 就代表了这个待估计的正面向上的概率值。

随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 的取值 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 表示抛掷 ![[公式]](https://www.zhihu.com/equation?tex=k) 次硬币，正面向上的次数，那么这个概率就表示为： ![[公式]](https://www.zhihu.com/equation?tex=P%28%5C%7BX%3Dx_i%5C%7D%29%3DC_%7Bk%7D%5E%7Bx_i%7D%5Ctheta%5E%7Bx_i%7D%281-%5Ctheta%29%5E%7Bk-x_i%7D) 。

这里需要注意的是， ![[公式]](https://www.zhihu.com/equation?tex=k) 和 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 都是指定的、已知的，而参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 是一个未知的参数。因此在这个大的背景下，抛掷 ![[公式]](https://www.zhihu.com/equation?tex=k) 次，其中有 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 次向上的概率是一个关于未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的函数，我们把他写作是 ![[公式]](https://www.zhihu.com/equation?tex=P%28%5C%7BX%3Dx_i%5C%7D%29%3Dp%28x_i%3B%5Ctheta%29) 。

概况的说：概率密度函数PMF是一个关于待估参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的函数。

那么此时，我们做 ![[公式]](https://www.zhihu.com/equation?tex=n) 次这种实验，每次实验中，都是连续抛掷 ![[公式]](https://www.zhihu.com/equation?tex=k) 次硬币，统计正面出现的次数，这样就能取得一系列的样本： ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2C...%2Cx_n) ，这些样本的取值之间满足相互独立，那么这一串样本取得上述取值 ![[公式]](https://www.zhihu.com/equation?tex=%5C%7BX_1%3Dx_1%2CX_2%3Dx_2%2CX_3%3Dx_3%2C...%2CX_n%3Dx_n%5C%7D) 的联合概率为：

![[公式]](https://www.zhihu.com/equation?tex=p%28x_1%3B%5Ctheta%29%5Ccdot+p%28x_2%3B%5Ctheta%29%5Ccdot+p%28x_3%3B%5Ctheta%29%5Ccdot+...%5Ccdot+p%28x_n%3B%5Ctheta%29) ，用连乘符号写起来就是 ：

![[公式]](https://www.zhihu.com/equation?tex=%5Cprod_%7Bi%3D1%7D%5E%7Bn%7Dp%28x_i%3B%5Ctheta%29)

这是一个通用的表达式，实际上，你别看他表达式是长长的一串，实际上他的未知数就是一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) ，而其他的 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 都是已知的样本值，因此我们说 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的取值，完全决定了这一连串样本取值的联合概率。

由此，我们更换一个更加有针对性的写法：

![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DL%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7Dp%28x_i%3B%5Ctheta%29)

那么， ![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DL%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta%29) 就是这一串已知样本值 ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2C...%2Cx_n) 的似然函数，他描述了取得这一串指定样本值的概率值，而这个概率值完全由未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 决定。这就是似然函数的由来。

当然如果 ![[公式]](https://www.zhihu.com/equation?tex=X) 是一个连续型的随机变量，我们只要相应的把离散型的概率质量函数替换成连续型的概率密度函数即可：

![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DL%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7Df%28x_i%3B%5Ctheta%29)

最大似然函数的图像类似如下的形状

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210618092218.png" height="300">

### 1.3.3 最大似然估计的思想

显然，似然函数 ![[公式]](https://www.zhihu.com/equation?tex=L%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta%29) 指的就是随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 取到指定的这一组样本值： ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2C...%2Cx_n) 时的概率的大小。当未知的待估计的参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 取不同的值时，计算出来的概率的值会发生变化。

例如，当 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta%3D%5Ctheta_0) 时，似然函数 ![[公式]](https://www.zhihu.com/equation?tex=L%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta_0%29) 的取值为0或趋近于0，那么意味着，当 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta%3D%5Ctheta_0) 时，随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 取得这一组样本 ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2C...%2Cx_n) 的概率为0，即压根儿不可能得到这一组样本值，或可能性非常非常小，那么你肯定不会觉得参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 应该能够取 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_0) 这个数。

那么当 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 取 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2) 两种不同的值时，似然函数的值 ![[公式]](https://www.zhihu.com/equation?tex=L%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta_1%29+%5Cgt+L%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta_2%29) ，意味着，当 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta%3D%5Ctheta_1) 时，随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 取得这一组指定样本的概率要更大一些，换句话说， ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 取 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1) 比取 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2) 有更大的可能获得这一组样本值 ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2C...%2Cx_n) ，那么当你面对这一组已经获得的采样值，在 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_2) 当中二选一作为估计值的时候，倾向于选择使似然函数取值更大的估计值，就是再自然不过的了。

这里就是盒子摸球试验中，我们选择一号盒子，丢硬币试验中，我们选择第三枚硬币的原因。

那么更进一步，跳出前面几个引导例子的限制，当我们的未知参数选择的余地更大时，比如我们的未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 是对一个概率值的估计，那么他的取值范围就是一个在 ![[公式]](https://www.zhihu.com/equation?tex=%5B0%2C1%5D) 之间取值的连续变量，如果是估计总体的方差，那么他的范围就是非负数，如果估计的是总体的均值，那么他的范围就是全体实数了。

此时我们要做的就是在未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的取值范围 ![[公式]](https://www.zhihu.com/equation?tex=%5CTheta) 中选取使得似然函数 ![[公式]](https://www.zhihu.com/equation?tex=L%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta%29) 能够取得最大值的 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D) ，作为未知参数的估计值，由于 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D) 使得似然函数取值达到最大，因此 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D) 就是未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的最大似然估计。

换句话说，未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 取估计值 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D) 时获取到这组已知样本 ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2C...%2Cx_n) 的可能性比取其他任何值时都要大，在这种思维框架下，我们有什么理由不用他呢？

### 1.3.4 最大似然估计值的计算

那么接下来，问题就到了如何求解这个最大似然估计值了。问题转换为一个求最值的问题：

即：在给定概率模型和一组相互独立的观测样本 ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2C...%2Cx_n) 的基础上，求解使得似然函数 ![[公式]](https://www.zhihu.com/equation?tex=+L%28%5Ctheta%29%3DL%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7Dp%28x_i%3B%5Ctheta%29) 取得最大值的未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的取值。当然，如果是连续型随机变量，则把似然函数替换成 ![[公式]](https://www.zhihu.com/equation?tex=L%28%5Ctheta%29%3DL%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Ctheta%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7Df%28x_i%3B%5Ctheta%29) 即可。

那么下面问题就变得很直接了，对似然函数求导使得导数为$0$的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的取值，就是我们要找的最大似然估计值 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D) 。

这个连乘的函数求导数比较复杂，由于函数 ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=ln%28g%28x%29%29) 的单调性是保持一致的。因此我们可以选择把似然函数 ![[公式]](https://www.zhihu.com/equation?tex=L%28x%29) 转化为 ![[公式]](https://www.zhihu.com/equation?tex=ln%28L%28x%29%29) ，这样连乘就变成了连加：

![[公式]](https://www.zhihu.com/equation?tex=ln%28L%28%5Ctheta%29%29%3Dln%28%5Cprod_%7Bi%3D1%7D%5E%7Bn%7Dp%28x_i%3B%5Ctheta%29%29%3Dln%28p%28x_1%3B%5Ctheta%29%29%5Ccdot+ln%28p%28x_2%3B%5Ctheta%29%29%5Ccdot++...+%5Ccdot+ln%28p%28x_n%3B%5Ctheta%29%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dln%28p%28x_i%3B%5Ctheta%29%29+)

此时再对他进行求导就变得容易了，如果方程有唯一解，且是极大值点，那么我们就求得了最大似然估计值。

如果有多个未知参数需要我们去估计呢？那也好办，用上偏导数就可以了：

![[公式]](https://www.zhihu.com/equation?tex=ln%28L%28%5Ctheta_1%2C%5Ctheta_2%2C...%2C%5Ctheta_k%29%29%3Dln%28%5Cprod_%7Bi%3D1%7D%5E%7Bn%7Dp%28x_i%3B%5Ctheta_1%2C%5Ctheta_2%2C...%2C%5Ctheta_k%29%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dln%28p%28x_i%3B%5Ctheta_1%2C%5Ctheta_2%2C...%2C%5Ctheta_k%29%29)

为了使得 ![[公式]](https://www.zhihu.com/equation?tex=lnL) 达到最大，我们对每一个待估计的未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_i) ，都去求偏导数，并建立方程组：

![[公式]](https://www.zhihu.com/equation?tex=+%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+lnL%7D%7B%5Cpartial+%5Ctheta_1%7D%3D0+%5C%5C+%5Cfrac%7B%5Cpartial+lnL%7D%7B%5Cpartial+%5Ctheta_2%7D%3D0+%5C%5C+......%5C%5C+%5Cfrac%7B%5Cpartial+lnL%7D%7B%5Cpartial+%5Ctheta_k%7D%3D0+%5C%5C+%5Cend%7Baligned%7D+%5Cright.+)

解得这个方程组就可以了。

### 1.3.5 最大似然估计的例子

说了这么多的理论方法，最后我们还是来看看实际例子中的估计方法：

#### 1.**3.5.1 简单案例热身**

第一个例子还是抛硬币的例子，我们的硬币正反面不规则，我们想要估计他正面向上的概率 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) ，我们连续抛掷10次，每一次抛掷的结果形成的样本序列如下：

正，正，正，反，反，正，反，正，正，反

很显然，每次抛掷的过程是都是彼此独立的，并且 ![[公式]](https://www.zhihu.com/equation?tex=X) 是一个伯努利随机变量。我们知道： ![[公式]](https://www.zhihu.com/equation?tex=p%28%5C%7Bx_i%3D%E6%AD%A3%5C%7D%29%3D%5Ctheta) ， ![[公式]](https://www.zhihu.com/equation?tex=p%28%5C%7Bx_i%3D%E5%8F%8D%5C%7D%29%3D1-%5Ctheta) ，那么这组观测数据的似然函数为：

![[公式]](https://www.zhihu.com/equation?tex=L%28x_1%2Cx_2%2C...%2Cx_%7B10%7D%3B%5Ctheta%29%3D%5Ctheta%5E3%281-%5Ctheta%29%5E2%5Ctheta%281-%5Ctheta%29%5Ctheta%5E2%281-%5Ctheta%29%3D%5Ctheta%5E6%281-%5Ctheta%29%5E4)

将其转换为对数似然函数：

![[公式]](https://www.zhihu.com/equation?tex=ln%28L%28x_1%2Cx_2%2C...%2Cx_%7B10%7D%3B%5Ctheta%29%29%3Dln%28%5Ctheta%5E6%281-%5Ctheta%29%5E4%29%3D6ln%5Ctheta%2B4ln%281-%5Ctheta%29)

然后对对数似然函数求导：

![[公式]](https://www.zhihu.com/equation?tex=ln%27%28L%28x_1%2Cx_2%2C...%2Cx_%7B10%7D%3B%5Ctheta%29%29%3D%286ln%5Ctheta%2B4ln%281-%5Ctheta%29%29%27+%3D%5Cfrac%7B6%7D%7B%5Ctheta%7D%2B%5Cfrac%7B4%7D%7B%5Ctheta-1%7D%3D%5Cfrac%7B10%5Ctheta-6%7D%7B%5Ctheta%28%5Ctheta-1%29%7D)

让对数似然函数的导数为0：

![[公式]](https://www.zhihu.com/equation?tex=ln%27%28L%28x_1%2Cx_2%2C...%2Cx_%7B10%7D%3B%5Ctheta%29%29%3D%5Cfrac%7B10%5Ctheta-6%7D%7B%5Ctheta%28%5Ctheta-1%29%7D%3D0)

得到最大似然估计值 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D%3D%5Cfrac%7B6%7D%7B10%7D)

#### 1.**3.5.2 单参数最大似然估计**

再看一个指数分布的例子：

比如在一个柜台前，相邻顾客的到达时间的时间间隔服从参数为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的指数分布，我们获取了一组上述时间间隔的样本 ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2C...%2Cx_n) ，下面来运用最大似然估计的方法来估计未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 。

首先依据指数分布，得到： ![[公式]](https://www.zhihu.com/equation?tex=f%28x_i%3B%5Ctheta%29%3D%5Ctheta+e%5E%7B-%5Ctheta+x_i%7D)

紧接着，得到对数似然函数：

![[公式]](https://www.zhihu.com/equation?tex=ln%28L%28x_1%2Cx_2%2C...%2Cx_n%3B%5Ctheta%29%29%3Dln%28%5Cprod_%7Bi-1%7D%5E%7Bn%7D%5Ctheta+e%5E%7B-%5Ctheta+x_i%7D%29%3D%5Csum_%7Bi%3D1%7D%5Enln%5Ctheta+e%5E%7B-%5Ctheta+x_i%7D%3D%5Csum_%7Bi%3D1%7D%5Enln%5Ctheta+%2B%5Csum_%7Bi%3D1%7D%5Enln+e%5E%7B-%5Ctheta+x_i%7D)

![[公式]](https://www.zhihu.com/equation?tex=%3D%5Csum_%7Bi%3D1%7D%5Enln%5Ctheta+-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Ctheta+x_i%3D%5Csum_%7Bi%3D1%7D%5Enln%5Ctheta+-%5Ctheta%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_i%3Dnln%5Ctheta+-%5Ctheta%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_i)

此时，我们对这个对数似然函数求导，来获取未知参数的最大似然估计值 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D) ，也很简单：

![[公式]](https://www.zhihu.com/equation?tex=ln%27%28L%28x_1%2Cx_2%2C...%2Cx_n%3B%5Ctheta%29%29%3D%28nln%5Ctheta+-%5Ctheta%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_i%29%27%3D%5Cfrac%7Bn%7D%7B%5Ctheta%7D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_i%3D0)

则有： ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7Bn%7D%7B%5Ctheta%7D%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_i)

那么未知参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的最大似然估计值 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D%3D%5Cfrac%7Bn%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_i%7D) 。

#### 1.**3.5.3 多参数最大似然估计**

上面是单参数的最大似然估计的例子，那么下面我们再来看看多参数的例子，这里我们从一个服从参数为 ![[公式]](https://www.zhihu.com/equation?tex=%28%5Cmu%2C%5Csigma%5E2%29) 的正态分布当中获取一组采样值： ![[公式]](https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3%2C...%2Cx_n) ，通过这组采样值，我们来求得两个参数的最大似然估计值：

首先写出似然函数：

![[公式]](https://www.zhihu.com/equation?tex=L%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Cmu%2C+%5Csigma%5E2%29%3D%5Cprod_%7Bi%3D1%7D%5En%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7De%5E%7B-%5Cfrac%7B%28x_i-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D%7D%3D%282%5Cpi%5Csigma%5E2%29%5E%7B-%5Cfrac%7Bn%7D%7B2%7D%7De%5E%7B-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Cmu%29%5E2%7D)

写成对数似然函数的形式：

![[公式]](https://www.zhihu.com/equation?tex=ln%28L%28x_1%2Cx_2%2Cx_3%2C...%2Cx_n%3B%5Cmu%2C+%5Csigma%5E2%29%29%3Dln%28%282%5Cpi%5Csigma%5E2%29%5E%7B-%5Cfrac%7Bn%7D%7B2%7D%7De%5E%7B-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Cmu%29%5E2%7D%29)

![[公式]](https://www.zhihu.com/equation?tex=%3Dln%282%5Cpi%5Csigma%5E2%29%5E%7B-%5Cfrac%7Bn%7D%7B2%7D%7D%2Bln%28e%5E%7B-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Cmu%29%5E2%7D%29%3D-%5Cfrac%7Bn%7D%7B2%7Dln2%5Cpi-%5Cfrac%7Bn%7D%7B2%7Dln%28%5Csigma%5E2%29-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Cmu%29%5E2)

由于这里有两个待估计的参数，我们分别对其进行求偏导， ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%5E2) 我们把他看作是一个整体即可：

![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial%28+-%5Cfrac%7Bn%7D%7B2%7Dln2%5Cpi-%5Cfrac%7Bn%7D%7B2%7Dln%28%5Csigma%5E2%29-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Cmu%29%5E2%29%7D%7B%5Cpartial+%5Cmu%7D%3D0+%5C%5C+%5Cfrac%7B%5Cpartial%28+-%5Cfrac%7Bn%7D%7B2%7Dln2%5Cpi-%5Cfrac%7Bn%7D%7B2%7Dln%28%5Csigma%5E2%29-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Cmu%29%5E2%29%7D%7B%5Cpartial+%5Csigma%5E2%7D%3D0++%5C%5C+%5Cend%7Baligned%7D+%5Cright.+)

第一个求偏导数的式子中，我们可以得出：![[公式]](https://www.zhihu.com/equation?tex=2%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Cmu%29%3D0)

则有： ![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5En+%5Cmu+%3D+%5Csum_%7Bi%3D1%7D%5En+x_i)

即 ![[公式]](https://www.zhihu.com/equation?tex=n+%5Cmu%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+x_i+%5CRightarrow+%5Chat%7B%5Cmu%7D%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5Enx_i) ，即得到了均值的最大似然估计值。

第二个求偏导的式子中，注意我们是把 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%5E2) 看作是一个整体，因此可以得出：

![[公式]](https://www.zhihu.com/equation?tex=-%5Cfrac%7Bn%7D%7B2%5Csigma%5E2%7D%2B%5Cfrac%7B1%7D%7B2%5Csigma%5E4%7D%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Cmu%29%5E2%3D0)

得到： ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Csigma%5E2%7D%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Cmu%29%5E2) ，而这里我们要带入 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 的最大似然估计值 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmu%7D) ，最终得到了方差的最大似然估计值：![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Csigma%5E2%7D%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28x_i-%5Chat%7B%5Cmu%7D%29%5E2)

总体方差的最大似然估计值的分母是 ![[公式]](https://www.zhihu.com/equation?tex=n) 而不是 ![[公式]](https://www.zhihu.com/equation?tex=n-1) ，因此他不是一个无偏估计量。但是可以说他是渐近无偏的，因为随着 ![[公式]](https://www.zhihu.com/equation?tex=n) 不断增大，他和无偏估计量逐渐趋向一致。

## 1.4 最大似然估计总结

<table><tr><td></td><td><center>概率</center></td><td><center>似然</center></td><td><center>最大似然估计</center></td></tr><tr><td>定义</td><td>在特定环境下某件事情发生的可能性</td><td>结果已知的情况下，该事件在不同条件下发生的可能性</td><td></td></tr><tr><td>理解</td><td>结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性</td><td>在确定的结果下去推测产生这个结果的可能环境（参数）</td><td></td></tr><tr><td>案例</td><td>抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%</td><td>假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%</td><td></td></tr><tr><td></td><td>这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；</td><td></td><td></td></tr><tr><td>用处</td><td></td><td>根据结果来判断这个事情本身的性质（参数）</td><td>“模型已定，参数未知”<br>    提供了一种给定观察数据来评估模型参数的方法</td></tr><tr><td></td><td>概率越大说明这件事情越可能会发生；</td><td>似然函数的值越大说明该事件在对应的条件下发生的可能性越大。</td><td></td></tr><tr><td>求解过程</td><td></td><td>最大似然估计的一般求解过程：<br>  　　（1） 写出似然函数；<br>  　　（2） 对似然函数取对数，并整理；<br>  　　（3） 求导数 ；<br>  　　（4） 解似然方程</td><td>我们平时所称的最大似然为最大的对数平均似然，即：</td></tr></table>

# 2. 逻辑回归

## 2.1 从线性回归说起

线性回归的表达式：
$$
f(x)=w^{T}x+b
$$
线性回归对于给定的输入$x$ ，输出的是一个数值 $y$ ，因此它是一个解决回归问题的模型。

假设我们盖房子要挑一些木材，木材的长度必须满足某个标准。于是我们就会根据该标准将木材划分为两类。小于标准的为不合格，大于标准的为合格，如下图所示。其中，$X$轴表示木材长度，$Y$轴表示是否合格。圆点表示已分类的木材。

对于这样的问题，用线性回归显然就不合适。因为它的输出不是连续的线性关系，而是非$0$即$1$的逻辑关系。所以很难拟合出一个好的效果，如图**黑线**所示。

**逻辑回归**的目的就是解决这类分类问题。

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210616154319.jpg)



## 2.2 逻辑回归理论部分

### 2.2.1 什么是逻辑回归

逻辑回归是一种利用样本数据求取逻辑模型，并用于新数据进行分类的一种方法。在逻辑回归中，数据样本的假设模型不再是线性的。而是非线性的逻辑模型$sigmod$，如下图所示。输入样本数据，输出该样本属于某个类的概率。

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210616154339.jpg)



逻辑回归模型实质上是一种复合变换。先对输入进行一次线性变换，再将线性变换的结果作为输入进行一次非线性变换，最后将输入映射到$0\sim 1$的范围，如下图所示。

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210616154358.jpg)

2.2.2 逻辑回归如何进行分类

通过逻辑回归模型，输出结果被压缩到$0\sim 1$的$S$形曲线中，其含义相当于样本为正的概率。

然后以50%的概率为分界线，将大于50%的选择为正样本，也就是$1$；小于50%的选择为负样本，也就是$0$，即可完成分类。如下图所示。

<table>
    <tr>
        <td>
            <center>
              <img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210616171820.png">
            </center>
        </td>
        <td>
            <center>
                             <img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210616154430.png">
        </center>
    </td>
</tr>


### 2.2.3 逻辑回归损失函数

逻辑回归，通常作为分类算法，只可以解决二分类问题。最终得出的结果是一个概率值。

假设只有两个标签$1$和$0$， ![[公式]](https://www.zhihu.com/equation?tex=y_n+%5Cin%5C%7B0%2C+1%5C%7D) 。我们把采集到的任何**一组样本**看做**一个事件**的话，那么这个事件发生的概率假设为$p$。我们的模型y的值等于标签为$1$的概率也就是$p$。

![[公式]](https://www.zhihu.com/equation?tex=P_%7By%3D1%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+%3D+p)

因为标签不是$1$就是$0$，因此标签为$0$的概率就是： ![[公式]](https://www.zhihu.com/equation?tex=P_%7By%3D0%7D+%3D+1-p) 。

我们把单个样本看做一个事件，那么这个事件发生的概率就是：

![[公式]](https://www.zhihu.com/equation?tex=P%28y%7C%5Cbm%7Bx%7D%29%3D%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+p%2C+y%3D1+%5C%5C+1-p%2Cy%3D0+%5Cend%7Baligned%7D+%5Cright.)

这个函数不方便计算，它等价于如下的等式(交叉熵):

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210617193412.svg)

解释下这个函数的含义，我们采集到了一个样本 ![[公式]](https://www.zhihu.com/equation?tex=%28%5Cbm%7Bx_i%7D%2Cy_i%29) 。对这个样本，它的标签是 ![[公式]](https://www.zhihu.com/equation?tex=y_i) 的概率是 ![[公式]](https://www.zhihu.com/equation?tex=p%5E%7By_i%7D%281-p%29%5E%7B1-%7By_i%7D%7D) 。 （当$y=1$，结果是$p$；当$y=0$，结果是$1-p$)。

如果我们采集到了一组数据一共$N$个， ![[公式]](https://www.zhihu.com/equation?tex=%5C%7B%28%5Cbm%7Bx%7D_1%2Cy_1%29%2C%28%5Cbm%7Bx%7D_2%2Cy_2%29%2C%28%5Cbm%7Bx%7D_3%2Cy_3%29...%28%5Cbm%7Bx%7D_N%2Cy_N%29%5C%7D) ，这个合成在一起的合事件发生的总概率怎么求呢？其实就是将每一个样本发生的概率相乘就可以了，即采集到这组样本的概率：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P_%7B%E6%80%BB%7D+%26%3D+P%28y_1%7C%5Cbm%7Bx%7D_1%29P%28y_2%7C%5Cbm%7Bx%7D_2%29P%28y_3%7C%5Cbm%7Bx%7D_3%29....P%28y_N%7C%5Cbm%7Bx%7D_N%29+%5C%5C++%26%3D+%5Cprod_%7Bn%3D1%7D%5E%7BN%7Dp%5E%7By_n%7D%281-p%29%5E%7B1-y_n%7D++%5Cend%7Baligned%7D)

注意![[公式]](https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB+%7D) 是一个函数，并且未知的量只有 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) （在$p$里面)。

由于连乘很复杂，我们通过两边取对数来把连乘变成连加的形式，即：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+F%28%5Cbm%7Bw%7D%29%3Dln%28P_%7B%E6%80%BB%7D+%29++%26%3D+ln%28%5Cprod_%7Bn%3D1%7D%5E%7BN%7Dp%5E%7By_n%7D%281-p%29%5E%7B1-y_n%7D+%29+%5C%5C+%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7Dln+%28p%5E%7By_n%7D%281-p%29%5E%7B1-y_n%7D%29+%5C%5C+%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%28y_n+ln+%28p%29+%2B+%281-y_n%29ln%281-p%29%29+%5Cend%7Baligned%7D+)

其中， ![[公式]](https://www.zhihu.com/equation?tex=p+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+)

这个函数 ![[公式]](https://www.zhihu.com/equation?tex=F%28%5Cbm%7Bw%7D%29) 又叫做它的**损失函数**。损失函数可以理解成衡量我们当前的模型的输出结果，跟实际的输出结果之间的差距的一种函数。这里的损失函数的值等于事件发生的总概率，我们希望它越大越好。但是跟损失的含义有点儿违背，因此也可以在前面取个负号。

### 2.2.4 最大似然估计求解参数

我们在真实世界中并不能直接看到概率是多少，我们只能观测到事件是否发生。也就是说，我们只能知道一个样本它实际的标签是$1$还是$0$。那么我们如何估计参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 跟$b$的值呢？

最大似然估计，就是一种估计参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 的方法。在这里如何使用MLE来估计 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 呢？

在上一节，我们知道损失函数 ![[公式]](https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89) 是正比于总概率 ![[公式]](https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB%7D) 的，而 ![[公式]](https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89) 又只有一个变量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 。也就是说，通过改变 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 的值，就能得到不同的总概率值 ![[公式]](https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB%7D) 。那么当我们选取的某个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A) 刚好使得总概率 ![[公式]](https://www.zhihu.com/equation?tex=P_%7B%E6%80%BB%7D) 取得最大值的时候。我们就认为这个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A) 就是我们要求得的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 的值，这就是最大似然估计的思想。

现在我们的问题变成了，找到一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A) ，使得我们的总事件发生的概率，即损失函数 ![[公式]](https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89) 取得最大值，这句话用数学语言表达就是：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%5E%2A%7D+%3D+arg%5Cmax_%7Bw%7DF%28%5Cbm%7Bw%7D%29+%3D+-arg%5Cmin_%7Bw%7DF%28%5Cbm%7Bw%7D%29+)

###  2.2.5 求![[公式]](https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89)的梯度![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89)

**梯度的定义**

我们知道对于一个一维的标量x，它有导数 ![[公式]](https://www.zhihu.com/equation?tex=x%27) 。

对一个多维的向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D+%3D+%28x_1%2Cx_2%2Cx_3%2C..%2Cx_n%29) 来说，它的导数叫做梯度，也就是分别对于它的每个分量求导数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D%27+%3D+%28x_1%27%2Cx_2%27%2Cx_3%27%2C..%2Cx_n%27%29) 。

为了求出 ![[公式]](https://www.zhihu.com/equation?tex=F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89)的梯度![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89)，我们需要做一些准备工作。首先，我们需要知道向量是如何求导的。我们只要记住几个结论就行了：对于一个矩阵 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7BA%7D) 乘以一个向量的方程 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7BA%7D%5Cbm%7Bx%7D) ，对向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 求导的结果是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7BA%7D%5ET) 。在这里我们把函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7BA%7D%5Cbm%7Bx%7D) 对 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 求梯度简单记作 ![[公式]](https://www.zhihu.com/equation?tex=%EF%BC%88%5Cbm%7BA%7D%5Cbm%7Bx%7D%EF%BC%89%27) 。因此![[公式]](https://www.zhihu.com/equation?tex=%EF%BC%88%5Cbm%7BA%7D%5Cbm%7Bx%7D%EF%BC%89%27+%3D+%5Cbm%7BA%7D%5ET) , 推论是 ![[公式]](https://www.zhihu.com/equation?tex=%EF%BC%88%5Cbm%7Bx%7D%5ET%5Cbm%7BA%7D%EF%BC%89%27+%3D+%5Cbm%7BA%7D) ，我们把 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D%2C%5Cbm%7Bw%7D%5ET) 代入进去，可以知道 ![[公式]](https://www.zhihu.com/equation?tex=%28%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%29%27+%3D+%5Cbm%7Bx%7D) 。

然后求 ![[公式]](https://www.zhihu.com/equation?tex=1-p) 的值：

![[公式]](https://www.zhihu.com/equation?tex=1-p%3D%5Cfrac%7Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D%7B+1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D)

$p$是一个关于变量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 的函数，我们对$p$求导，通过链式求导法则，慢慢展开可以得：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++p%27+%3D+f%27%28%5Cbm%7Bw%7D%29%26%3D+%28%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+%29%27+%5C%5C+%26%3D+-%5Cfrac%7B1%7D%7B+%281%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%29%5E2%7D+++%C2%B7+%28+1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%29%27+%5C%5C+%26%3D+-%5Cfrac%7B1%7D%7B+%281%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%29%5E2%7D+++%C2%B7+e%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D++%C2%B7+%28-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%29%27+%5C%5C+%26%3D+-%5Cfrac%7B1%7D%7B+%281%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%29%5E2%7D+++%C2%B7+e%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D++%C2%B7+%28-%5Cbm%7Bx%7D+%29+%5C%5C+%26%3D+%5Cfrac%7Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D%7B+%281%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%29%5E2%7D+%C2%B7+++%5Cbm%7Bx%7D+%5C%5C+%26%3D++%5Cfrac%7B1%7D%7B+1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D++++%C2%B7+%5Cfrac%7Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D%7B+1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%7D++%C2%B7+%5Cbm%7Bx%7D+%5C%5C+%26%3D+p%281-p%29%5Cbm%7Bx%7D+%5Cend%7Baligned%7D)

上面都是我们做的准备工作，总之我们得记住： ![[公式]](https://www.zhihu.com/equation?tex=p%27+%3D+p%281-p%29%5Cbm%7Bx%7D) , 并且可以知道 ![[公式]](https://www.zhihu.com/equation?tex=%281-p%29%27+%3D+-p%281-p%29%5Cbm%7Bx%7D) 。



下面我们正式开始对 ![[公式]](https://www.zhihu.com/equation?tex=F%28%5Cbm%7Bw%7D%29) 求导，求导的时候请始终记住，我们的变量只有 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) ，其他的什么 ![[公式]](https://www.zhihu.com/equation?tex=y_n%2C%5Cbm%7Bx%7D_n+) 都是已知的，可以看做常数。

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89%26+%3D+%5Cnabla+%EF%BC%88+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%28y_n+ln+%28p%29+%2B+%281-y_n%29ln%281-p%29%29+%EF%BC%89%5C%5C+%26%3D+%5Csum+%28+y_n+ln%27%28p%29+%2B+%281-y_n%29+ln%27%281-p%29%29+%5C%5C+%26%3D+%5Csum%28+%28y_n+%5Cfrac%7B1%7D%7Bp%7Dp%27%29%2B%281-y_n%29%5Cfrac%7B1%7D%7B1-p%7D%281-p%29%27%29+%5C%5C+%26%3D+%5Csum%28y_n%281-p%29%5Cbm%7Bx%7D_n+-+%281-y_n%29p%5Cbm%7Bx%7D_n%29+%5C%5C+%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%28y_n-p%29%5Cbm%7Bx%7D_n%7D+%5Cend%7Baligned%7D)

终于，我们求出了梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89) 的表达式了，现在我们再来看看它长什么样子：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89%26%3D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%28y_n-p%29%5Cbm%7Bx%7D_n%7D++%5Cend%7Baligned%7D)

它是如此简洁优雅，这就是我们选取sigmoid函数的原因之一。当然我们也能够把p再展开，即：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89%26%3D++%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D+++%5Cend%7Baligned%7D)



### 2.2.6 梯度下降法（GD）与随机梯度下降法（SGD）

在这里我先解释一个问题：**为什么可以用梯度下降法？**

因为逻辑回归的损失函数L是一个连续的凸函数（conveniently convex）。这样的函数的特征是，它只会有一个全局最优的点，不存在局部最优。对于GD跟SGD最大的潜在问题就是它们可能会陷入局部最优。然而这个问题在逻辑回归里面就不存在了，因为它的损失函数的良好特性，导致它并不会有好几个局部最优。当我们的GD跟SGD收敛以后，我们得到的极值点一定就是全局最优的点，因此我们可以放心地用GD跟SGD来求解。

现在我们已经解出了损失函数 ![[公式]](https://www.zhihu.com/equation?tex=+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89)在任意 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 处的梯度 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89)，可是我们怎么算出来 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%5E%2A%7D) 呢？ 回到之前的问题，我们现在要求损失函数取最大值时候的![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%5E%2A%7D)的值：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%5E%2A%7D+%3D+arg%5Cmax_%7Bw%7DF%28%5Cbm%7Bw%7D%29+)

**梯度下降法(Gradient Descent)**，可以用来解决这个问题。核心思想就是先随便初始化一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_0) ，然后给定一个步长 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) ，通过不断地修改 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+) <- ![[公式]](https://www.zhihu.com/equation?tex=+++%5Cbm%7Bw%7D_%7Bt%7D) ，从而最后靠近到达取得最大值的点，即不断进行下面的迭代过程，直到达到指定次数，或者梯度等于0为止。

![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89)

**随机梯度下降法（Stochastic Gradient Descent）**，如果我们能够在每次更新过程中，加入一点点噪声扰动，可能会更加快速地逼近最优值。在SGD中，我们不直接使用 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89) ，而是采用另一个输出为随机变量的替代函数 ![[公式]](https://www.zhihu.com/equation?tex=G%28%5Cbm%7Bw%7D%29) :

![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta++G%28%5Cbm%7Bw%7D%29)

当然，这个替代函数 ![[公式]](https://www.zhihu.com/equation?tex=G%28%5Cbm%7Bw%7D%29)需要满足它的期望值等于![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89)，相当于这个函数围绕着 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89) 的输出值随机波动。

好了，那我们要怎么实现学习算法呢？其实很简单，注意我们GD求导每次都用到了所有的样本点，从1到N都参与梯度计算。

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cnabla+F%EF%BC%88%5Cbm%7Bw%7D%EF%BC%89%26%3D++%5Csum_%7Bn%3D1%7D%5E%7BN%7D%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D+%5Cend%7Baligned%7D)

在SGD中，我们每次只要均匀地、随机选取其中一个样本 ![[公式]](https://www.zhihu.com/equation?tex=%28%5Cbm%7Bx_i%7D%2Cy_i%29) ,用它代表整体样本，即把它的值乘以N，就相当于获得了梯度的无偏估计值，即 ![[公式]](https://www.zhihu.com/equation?tex=E%28G%28%5Cbm%7Bw%7D%29%29+%3D+%5Cnabla+F%28%5Cbm%7Bw%7D%29) ，因此SGD的更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta++N++%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D)

这样我们前面的求和就没有了，同时 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta++N) 都是常数， ![[公式]](https://www.zhihu.com/equation?tex=N) 的值刚好可以并入 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta+) 当中,因此SGD的迭代更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta+++%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%28%5Cbm%7Bx_i%7D%2Cy_i%29) 是对所有样本随机抽样的一个结果。



### 2.2.7 逻辑回归的可解释性

逻辑回归最大的特点就是**可解释性**很强。

在模型训练完成之后，我们获得了一组n维的权重向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) 跟偏差 $b$。

对于权重向量 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D)，它的每一个维度的值，代表了这个维度的特征对于最终分类结果的贡献大小。假如这个维度是正，说明这个特征对于结果是有正向的贡献，那么它的值越大，说明这个特征对于分类为正起到的作用越重要。

对于偏差$b (Bias)$，一定程度代表了正负两个类别的判定的容易程度。假如$b$是$0$，那么正负类别是均匀的。如果$b>0$，说明它更容易被分为正类，反之亦然。

根据逻辑回归里的权重向量在每个特征上面的大小，就能够对于每个特征的重要程度有一个量化的清楚的认识，这就是为什么说逻辑回归模型有着很强的解释性的原因。

### 2.2.8 决策边界

补充一个问题，逻辑回归的决策边界是否是线性的，相当于问曲线：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D+%3D+0.5)

是不是的线性的，我们可以稍微化简一下上面的曲线公式，得到：

![[公式]](https://www.zhihu.com/equation?tex=e%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D+%3D+1+%3D+e%5E%7B0%7D+%5C%5C+%E5%8D%B3++-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D+%3D+0)

我们得到了一个等价的曲线，显然它是一个超平面（它在数据是二维的情况下是一条直线）。

![img](https://pic3.zhimg.com/80/v2-3f2227d606b29be9619566b88d1f1912_720w.jpg)

### 2.2.9 逻辑回归理论小结

终于一切都搞清楚了，现在我们来理一理思路，首先逻辑回归模型长这样：

![[公式]](https://www.zhihu.com/equation?tex=y%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D%7D%7D)

其中我们不知道的量是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D) ，假设我们已经训练好了一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A) , 我们用模型来判断 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D_i) 的标签呢？很简单，直接将![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D_i)代入$y$中，求出来的值就是![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bx%7D_i)的标签是$1$的概率，如果概率大于$0.5$，那么我们认为它就是$1$类，否则就是$0$类。

那怎么得到 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A) 呢？

如果采用随机梯度下降法的话，我们首先随机产生一个![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D)的初始值 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_0) ,然后通过公式不断迭代从而求得![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D%5E%2A)的值：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbm%7Bw%7D_%7Bt%2B1%7D+%3D+%5Cbm%7Bw%7D_t+%2B+%5Ceta+++%7B%28y_n-+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Cbm%7Bw%7D%5ET%5Cbm%7Bx%7D_n%7D%7D+%29%5Cbm%7Bx%7D_n%7D)

每次迭代都从所有样本中随机抽取一个 ![[公式]](https://www.zhihu.com/equation?tex=%28%5Cbm%7Bx_i%7D%2Cy_i%29) 来代入上述方程。

## 2.3 从函数的角度理解交叉熵损失函数

<table>
    <tr>
        <td><center><img src = "https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210617100018.png"></center></td>
    	<td><center><img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210617100032.jpg"></center></td>
    </tr>
</table>

决策边界$z$：
$$
z=w^{T}x=xw
$$
$singmod$函数相当于把一条直线强行掰弯，使其值域在$(0,1)$之间。这样就完成从直线回归到逻辑回归的转变。

接下来再了解一下这个转换函数$singmod$：
$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$


当$z>$0时，$p>0.5$；当$z<0$时，$p<0.5$；这样很容易就能够完成$2$分类了。那么我们将直线回归代入：
$$
\begin{align}
&\hat{p}=\sigma(w^{t}x)=\frac{1}{1+e^-(w^{T}x)}\\
\\
&\hat{y}=
\begin{cases}
 1,\quad \hat{p}\ge0.5 \\
 0, \quad \hat{p}<0.5

\end{cases}
\end{align}
$$


那么问题来了：

对于给定的样本数据集$x$，$y$如何找到参数 $w$ ，使得使用这样的方式，可以最大程度地获得样本数据集$x$，对应的分类输出$y$？

通过前面的分析已经成功地把线性预测转换成了一个概率，那如何定义损失 函数呢？
$$
\begin{align}
&\hat{p}=\sigma(w^{t}x)=\frac{1}{1+e^-(w^{T}x)}\\
\\
&\hat{y}=
\begin{cases}
 1,\quad \hat{p}\ge0.5 \\
 0, \quad \hat{p}<0.5
\end{cases},
&cost=
\begin{cases}
 如果y=1,p越小，cost越大，也就是损失越大 \\
 如果y=0,p越大，cost越大，也就是损失越大
\end{cases}\\
\end{align}
$$


显然上面这个函数是一个分段函数，对于我们后续的研究很不友好，接下来就是数学最美的地方了，把上面的描述转换为一个统一的数学公式（交叉熵损失）：

我们先来看一下$y=-ln(x)$与$y=-ln(1-x)$的函数图像：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210617104456.jpeg" width="600" height="400">
$$
cost = \begin{cases}
-ln(\hat{p}),\quad if\quad y=1\\
-ln(1-\hat{p}),\quad if \quad y=0
\end{cases}
$$
那么把上面这个分段函数整合在一起就可以得到下面这个交叉熵损失函数了，这样把一个分段函数合在了一起，对后续的研究就方便多了。
$$
cost = -yln(\hat{p})-(1-y)ln(1-\hat(p))
$$


到此，就可以总结出Logistic Regression的损失函数：
$$
J(w)=-\sum_{i=1}^{n}(y_{i}ln(\hat{p_{i}})+(1-y_{i})ln(1-\hat{p_{i}}))
$$
将$\hat{p}$的表达式带入即可得到完整的损失函数：
$$
\begin{align}
&\hat{p}=\sigma(w^{t}x)=\frac{1}{1+e^-(w^{T}x)}\\
\\
&J(w)=-\sum_{i=1}^{n}(y_{i}ln(\sigma(w^{t}x))+(1-y_{i})ln(1-\sigma(w^{t}x)))\\
\\
&J(w)=-\sum_{i=1}^{n}(y_{i}ln(\frac{1}{1+e^-(w^{T}x)})+(1-y_{i})ln(1-\frac{1}{1+e^-(w^{T}x)}))
\end{align}
$$


这个函数是一个凸函数，他没有公式解，使用梯度下降法即可求解参数$w$。

## 2.4 逻辑回归向量化案例

### 2.4.1 数据初始化

以一个二维影响因素为例：
$$
\begin{align}
&x_{1}= 
\begin{pmatrix}
 x^{1}\quad
 x^{2}
\end{pmatrix},\quad
W=
\begin{pmatrix}
 w^{1}\\
 w^{2}
\end{pmatrix},\quad
b = w^{0}\\
\\
&x_{1\times3}={x}'=
\begin{pmatrix}
 1\quad
 x_{1}\quad
 x_{2}
\end{pmatrix},\quad
W_{3\times1}={W}'=
\begin{pmatrix}
 w^{0}\\
 w^{1}\\
 w^{2}
\end{pmatrix}\\
\\

&z_{i}=x_{i}W=(x_{i}^{0} \quad x_{i}^{1}\quad x_{i}^{3})
\begin{pmatrix}
 w^{0}\\
 w^{1}\\
 w^{2}
\end{pmatrix}
\end{align}
$$

### 2.4.2 模型及损失函数

假设数据集$Data$有十条记录则,$X$的维度为$10\times 3$,$Y$的维度为$10\times1$

数据：$X_{10\times3},\quad Y_{10\times1} ,\quad W_{3\times1}\\$

决策边界：$Z_{10\times1}=XW\quad $

非线性映射：$h(z)=\sigma(z)=\frac{1}{1+e^{-z}}$

sigmod导数：${h}'_{z}=\frac{d\sigma(z)}{dz}=h(1-h)$

$H$对$Z$求导：$\frac{dH}{dZ}={H}'_{10\times1}=H(1-H)$

概率预测：$\hat{P}_{10\times1}=H_{10\times1}=\frac{1}{1+e^{-Z}}=\frac{1}{1+e^{-(XW)}}$

损失函数：$Cost_{10\times1}=-Yln(H)-(1-Y)ln(1-H)$

链式求导关系：Cost-H-Z-WX

损失函数$C$对参数$W$求导
$$
\begin{align}
&\frac{\partial Cost}{\partial W} =
\frac{\partial C}{\partial H}*
\frac{\partial H}{\partial Z}*
\frac{\partial Z}{\partial W}=
X^{T}*\left\{ \left(-\frac{Y}{H}-\frac{1-Y}{1-H}*(-1)\right)*H(1-H)\right \}\\
\\
&\frac{\partial Cost}{\partial W}=
X^{T}*\left\{ \left(\frac{-Y*(1-H)+H(1-Y)}{H(1-H)} \right)*H(1-H)\right \}\\
\\
&\frac{\partial Cost}{\partial W}=X^{T}(-Y+YH+H-HY)\\
\\
&\frac{\partial Cost}{\partial W}_{3\times1}=X^{T}(H-Y)\\
\\
&\frac{\partial Cost}{\partial W}_{3\times1}=X^{T}(\frac{1}{1+e^{-(XW)}}-Y)
\end{align}
$$


### 2.4.3 梯度下降法求解参数$W$

1. 初始化$W$
2. 更新$W$:$W_{t+1}=W_{t}-\alpha*\frac{\partial Cost}{\partial W}$
3. 重复多次迭代

### 2.4.4 code

```python
#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
==================================================
@Project -> File   ：study-ml -> simple_log_r.py
@IDE    ：PyCharm
@Author ：jhong.tao
@Date   ：2021/6/17 18:37
@Desc   ：Logistic Regression
==================================================
"""

# import lib
import numpy as np
import matplotlib.pyplot as plt


# def load data
def loaddata(filename):
    file = open(filename)
    x = []
    y = []
    for line in file.readlines():
        line = line.strip().split()
        x.append([1,float(line[0]), float(line[1])])
        y.append(float(line[-1]))
    xmat = np.mat(x)
    ymat = np.mat(y).T
    file.close()
    return xmat, ymat


# w calc
def w_calc(xmat, ymat, alpha=0.001, maxIter=10001):
    # W init
    W = np.mat(np.random.randn(3,1))
    w_save = []
    # W update
    for i in range(maxIter):
        H = 1/(1+np.exp(-xmat*W))
        dw = xmat.T*(H-ymat) # dw:(3,1)
        W -= alpha * dw
        if i % 100 ==0:
            w_save.append([W.copy(),i])
    return W, w_save


# show
def re_show(xmat, ymat, wsave):
    for wi in wsave:
        plt.clf()
        w0 = wi[0][0,0]
        w1 = wi[0][1,0]
        w2 = wi[0][2,0]
        plotx1 = np.arange(2,6,0.01)
        plotx2 = -w0/w2-w1/w2*plotx1
        plt.plot(plotx1, plotx2, c='r', label='decision boundary')

        plt.scatter(xmat[:,1][ymat==0].A, xmat[:,2][ymat==0].A, marker='^', s=150,label='label=0')
        plt.scatter(xmat[:,1][ymat==1].A, xmat[:,2][ymat==1].A, s=150,label='label=1')
        plt.grid()
        plt.legend()
        plt.title('iter:%s'%np.str_(wi[1]))
        plt.pause(0.001)
        plt.show()


# implement
if __name__=='__main__':
    xmat, ymat= loaddata('..\ch00_dataset\lr_data.txt')
    print('xmat:', xmat, xmat.shape)
    print('ymat:', ymat, ymat.shape)
    W,w_save = w_calc(xmat, ymat, 0.001, 10000) # w save
    print('W:', W)
    re_show(xmat, ymat, w_save)

```

![image-20210617192056419](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210617192058.png)

## 2.5 逻辑回归肿瘤分类案例

### 2.5.1 数据集

data：https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210618080555.png)



### 2.5.2 code

```python
#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
==================================================
@Project -> File   ：study-ml -> logr_cancer
@IDE    ：PyCharm
@Author ：jhong.tao
@Date   ：2021/6/18 6:45
@Desc   ：Data https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/
==================================================
"""
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
from matplotlib import pyplot as plt


# 数据获取sh
def dataloader():
    # 获取数据并添加字段名
    column_name = ['userID', '肿块厚度', '细胞大小均匀度', '细胞形状均匀度', '边缘粘', '单一上皮细胞大小', '裸核', '乏味染色体', '正常核', '有丝分裂', '2良性/4恶性']

    cancer = pd.read_csv("..\ch00_dataset/breast-cancer-wisconsin.data", names=column_name)
    cancer.head()

    # 缺失值处理
    cancer = cancer.replace(to_replace="?", value=np.nan)
    cancer = cancer.dropna()

    # 数据集划分
    # 1> 提取特征数据与目标数据
    x = cancer.iloc[:, 1:-2]
    y = cancer.iloc[:, -1]
    # 2> 划分数据集
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

    # 标准化处理
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)
    return x_train, y_train, x_test, y_test


# 模型训练和评估
def model(x_train, y_train, x_test, y_test):

    # 创建一个逻辑回归估计器
    estimator = LogisticRegression()
    # 训练模型，进行机器学习
    estimator.fit(x_train, y_train)
    # 得到模型，打印模型回归系数，即权重值
    print("logist回归系数为:\n", estimator.coef_)
    # 模型评估
    # 方法1：真实值与预测值比对
    y_predict = estimator.predict(x_test)
    print("预测值为:\n", y_predict)
    print("真实值与预测值比对:\n", y_predict==y_test)
    # 方法2：计算准确率
    print("直接计算准确率为:\n", estimator.score(x_test, y_test))

    # 接上面的肿瘤预测代码

    # 打印精确率、召回率、F1 系数以及该类占样本数
    print("精确率与召回率为:\n", classification_report(y_test,y_predict, labels=[2, 4], target_names=["良性", "恶性"]))

    # 模型评估
    # ROC曲线与AUC值
    # 把输出的 2 4 转换为 0 或 1
    # y_test = np.where(y_test > 2, 1, 0)  # 大于2就变为1，否则变为0
    print("AUC值:\n", roc_auc_score(y_test, y_predict))

    # 预测结果生成混淆矩阵，confusion matrix
    target_names = ['begin', 'malignant']
    mat = confusion_matrix(y_test, y_predict)
    plt.title("predict-to-true confusion matrix")
    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=target_names, yticklabels=target_names)
    plt.xlabel('true label')
    plt.ylabel('predicted label')
    plt.show()


# main
if __name__ == '__main__':
    x_train, x_test, y_train, y_test = dataloader()
    model(x_train, x_test, y_train, y_test)

```

**混淆矩阵**

![image-20210618083148400](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210618083149.png)

### 2.5.3 分类评估方法

#### 2.5.3.1 精确率与召回率

##### 混淆矩阵

在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)

|            | 预测为正例 | 预测未假例 |
| ---------- | ---------- | ---------- |
| 真实为正例 | 真正例TP   | 伪反例FN   |
| 真实为假例 | 伪正例FP   | 真返例TN   |

##### 精确率(Precision)与召回率(Recall)

**精确率**：预测结果为正例样本中真实为正例的比例，（查的准不准）
$$
\frac{TP}{TP+TF}
$$


**召回率**：真实为正例的样本中预测结果为正例的比例，（查得全，对正样本的区分能力）

**TPR:正例的召回率**  所有真实类别为正例的样本中，预测类别也为正例的比例
$$
TPR=\frac{TP}{TP+FN}
$$
**FPR:假例的召回率：**所有真实类别为假例的样本中，预测类别为正例的比例
$$
FPR=\frac{FP}{FP+TN}
$$


**F1-score**，反映了模型的稳健型


$$
F_{1}=\frac{2TP}{2TP+FP+FN}
$$




**分类评估报告api**

sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )

y_true：真实目标值

y_pred：估计器预测目标值

labels:指定类别对应的数字

target_names：目标类别名称

return：每个类别精确率与召回率

#### 2.5.3.2 ROC曲线与AUC指标

**ROC曲线**

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210618085832.png" height="300">

**说明：**

1. 假值的召回率FPR作为横坐标
2. 真值的召回率TPR作为纵坐标
3. Threshold是阈值的意思
4. 阈值从0变化到1，才会得到不同的TPR金额FPR，才可以有很多的点，去绘制ROC曲线
5. 模型越好，蓝线越弯向于左上角
6. 这样可以看出模型的好坏，对比两个模型的ROC
7. 但是从ROC图形来看模型好不好是我们人工看的，计算机怎么识别呢？这就需要AUC指标了
8. 红色的这根线就是随便猜测的一个模型，最差的得到的就是红色的这个模型
9. ROC曲线的横轴就是FPRate，纵轴就是TPRate，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5

**AUC指标**

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210618085904.png" height="300">

**说明：**

1. AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率
2. AUC的最小值为0.5，最大值为1，取值越高越好，最终AUC的范围在[0.5, 1]之间，并且越接近1越好
3. AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器
4. 0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值
5. AUC只能用来评价二分类
6. AUC非常适合评价样本不平衡中的分类器性能

意义解释

1. AUC的意思是——Area Under roc Curve，就是ROC曲线的积分，也是ROC曲线下面的面积
2. 绘制ROC曲线的意义很明显，不断地把可能分错的情况扣除掉，从概率最高往下取的点，每有一个是负样本，就会导致分错排在它下面的所有正样本，所以要把它下面的正样本数扣除掉（1-TPR，剩下的正样本的比例）。总的ROC曲线绘制出来了，AUC就定了，分对的概率也能求出来了

**AUC计算API**

1. from sklearn.metrics import roc_auc_score
   1. 计算ROC曲线面积，即AUC值
   2. y_true：每个样本的真实类别，必须为0(反例),1(正例)标记
   3. y_score：预测得分，可以是正类的估计概率、置信值或者分类器方法的返回值

# 参考文献

说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。*

申明：仅用于学习交流使用，未经同意禁止商用、转载。

致谢：感谢参考文献中的各位作者。

1. [如何清楚明白的说明最大似然？](https://www.zhihu.com/question/35992368)
2. [参数估计(二).最大似然估计](https://zhuanlan.zhihu.com/p/55791843)
3. [如何理解似然函数?](https://www.zhihu.com/question/54082000)
4. [机器学习中的数学：概率统计](https://blog.csdn.net/weixin_43716250/category_10520681.html)
5. [似然函数与极大似然估计](http://fangs.in/post/thinkstats/likelihood/)
6. [通俗理解“最大似然估计”](https://zhuanlan.zhihu.com/p/334890990)
7. [如何通俗地理解概率论中的「极大似然估计法」?](https://www.zhihu.com/question/24124998/answer/1547063354)
8. [矩阵求导](https://blog.csdn.net/mounty_fsc/article/details/51588794)
9. [机器学习---逻辑回归](https://mp.weixin.qq.com/s/dQnf2NyUlAjOyIuGEjKKwQ)
10. [逻辑回归 logistics regression 公式推导](https://zhuanlan.zhihu.com/p/44591359)
11. [机器学习理论（八）逻辑回归](https://zhuanlan.zhihu.com/p/77868008)
12. [【机器学习面试总结】—— LR（逻辑回归）](https://zhuanlan.zhihu.com/p/100763009)
13. [逻辑回归](https://www.chenbinpeng.com/jiqixuexi/5_LogisticRegression/5_LogisticRegression.html)
14. [7分钟搞懂逻辑回归的来龙去脉](https://zhuanlan.zhihu.com/p/351244337)
15. [逻辑回归理解及公式推导](https://www.jianshu.com/p/0cfabca442d9)

