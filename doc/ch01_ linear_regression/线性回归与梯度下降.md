

# 机器学习概论

## 机器学习鸟瞰

![image-20210604224111677](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210604224111677.png)

## 传统机器学习鸟瞰

![image-20210604230151261](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210604230151261.png)

## 深度学习鸟瞰

![image-20210604223650339](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210604223650339.png)

## 机器学习的数学鸟瞰

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640">

# 线性回归

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/learner.jpg">

# 1. 回归与分类

监督式学习的应用场景总共有两个，**分类**（英文：Classification）和 回归（英文：Regression）：

![image-20210605100614076](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210605100614076.png)

与**分类算法**相对应的是**回归算法**，简而言之，**回归**就是预测一系列**连续**的值，**分类**就是预测一系列**离散**的值。

<img src="https://pic2.zhimg.com/v2-a2f33a0552199d0e5537c4c8e054e599_r.jpg" width=600>

<center>表1</center>

那么，这个回归究竟是什么意思呢？其实**回归算法**是相对**分类算法**而言的，与我们想要预测的目标变量y的值类型有关。如果**目标变量y是分类型变量**，如预测用户的性别（男、女），预测是否患病（是、否），那我们就需要用**分类算法**去拟合训练数据并做出预测；如果**y是连续型变量**，如预测用户的收入（4千，2万，10万……），预测患病的概率（1%，50%，99%……），我们则需要用**回归模型**。

**以下用一个简单的例子来说明：**

通过市场调查，我们得到一些房**屋面积和价格**的相关数据。我们想知道，**如果给一个新的房屋面积$130m^{2}$ ，能否根据已知的数据来预测其对应价格是多少呢**？如**表2**：

![preview](https://pic2.zhimg.com/v2-12e263923398e2cde1a256febb43386d_r.jpg)

<center>表2</center>

为了解决这个问题，我们引入**线性回归模型**。

# 2. 一元线性回归

## 2.1 一元线性回归引入

首先，画出已知数据的**散点图1**,其次，我们**模拟**出一条**直线**如散点**图2**，让已知的数据点尽量落在直线上或直线周围

<table>
    <tr>
        <td>
            <center>
                <img src="https://pic2.zhimg.com/80/v2-cfa9fb00efb5b2ad0267699516eac899_1440w.jpg" width=600>
            </center>
        </td>
         <td>
            <center>
                <img src="https://pic4.zhimg.com/80/v2-1996e65166b80e3f9d8f68efd75f24d3_1440w.jpg" width=600>
            </center>
        </td>
    <tr>
        <td width=600><center>图1</center></td>
        <td width=600><center>图2</center></td>
    </tr>
    </tr>

</table>

最后，我们求出这条直线模型对应的**函数公式**，然后代入$x=130$，即可求得其预测价格$f(x)$。

## 2.2 一元线性回归模型

**线性模型**就是一条**直线**： ![[公式]](https://www.zhihu.com/equation?tex=f%28x_i%29%3Dwx_i%2Bb) 。其中，$w$为系数，$b$为截距。

我们现在知道，**线性回归就是要找一条直线，并且让这条直线尽可能地拟合图中的数据点**。

那么如何得到$w$和$b$从而构造这个公式呢？估计如果让1000个人来画这条线就会有1000种画法，比如说：

<table>
    <tr>
        <td>
            <img src="https://pic2.zhimg.com/80/v2-3cb336840ca7508de522c5f87399c185_1440w.jpg" width=600>
        </td>
         <td>
            <img src="https://pic2.zhimg.com/80/v2-587a15b0d8b889b1fa57a815da4bf9dd_1440w.jpg" width=600>
        </td>
    </tr>
        <tr>
        <td width=600><center>图3</center></td>
        <td width=600><center>图4</center></td>
    </tr>
</table>

所以，我们**需要**一个**评判标准**，来评判哪条直线才是**最好**的。

由此，我们引入**损失函数**（**代价函数**）来作为评判标准。

## 2.3 一元线性回归 损失函数

接上面的*散点图3* 和*散点图4* 两种拟合情况。

对于散点图3的拟合直线 ![[公式]](https://www.zhihu.com/equation?tex=y%3D0.7925x%2B15.353) ，

以及散点图4的拟合直线 ![[公式]](https://www.zhihu.com/equation?tex=y%3D1.2452x-25) ，

到底哪一条直线才最“**合适**”呢？

由此我们引入**残差**，说白了就是**真实值和预测值间的差值**（也可以理解为**差距、距离**）。即算一下**实际房价**和根据拟合直线的**预测房价**之间的差距（距离）就行了。

当把**所有**实际房价和预测房价的差距（距离）算出来然后**求和**，我们就**能量化**预测房价和实际房价之间的**残差**。

例如**图5**中有很多红色小竖线，每一条就是实际房价和预测房价的**误差**（**距离**）。

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/v2-5985d35ac7498651312d70328c53391d_1440w.jpg" width="600">

<center>图5</center>

**残差公式**：

![image-20210603155951191](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210603155951191.png)

其中，$\hat{y}_{i}$是预测房价，$y_{i}$是真实房价。



**损失函数/残差平方和/均方误差（MSE/SSE）/欧氏距离之和**：

![image-20210603160312801](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210603160312801.png)

其中，$J(w,b)$是损失函数，$n$表示样本个数， ![[公式]](https://www.zhihu.com/equation?tex=f%28x_i%29) 是预测值， ![[公式]](https://www.zhihu.com/equation?tex=y_i) 是真实值。

**总结**：

**损失函数**是衡量回归模型误差的函数，也就是我们要的“**直线**”的**评价标准**。这个函数的值**越小**，说明直线越能拟合我们的数据。

到这里，我们通过损失函数公式

散点**图3**参数 ![[公式]](https://www.zhihu.com/equation?tex=w%3D0.7925%EF%BC%8Cb%3D15.353) ，

散点**图4**参数 ![[公式]](https://www.zhihu.com/equation?tex=w%3D1.2452%EF%BC%8Cb%3D-25) 。

算得散点图3的损失函数J要小于散点图4的损失函数$J(w,b)$。

所以，可以说明，**散点图3拟合的直线要比散点图4拟合的直线更“合适”**。

但是，我们不应该止步于此，我们要找的不是两者之间的更优解，而应该是所有拟合直线中的**最“合适”**。

由此，我们引出**最小二乘法**。

## 2.4 最小二乘法-参数求解

### 2.4.1 一元线性回归图式

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/learner.jpg" width=600>

### 2.4.2 一元线性回归数据集

**数据集D:**
$$
\begin{align}
&D=\{{(x_{1},y_{1}),(x_{2},y_{2})},\dots,(x_{n},y_{n})\}\\
\end{align}
$$
其中，$x_{i}$是一维标量，$y_{i}$也是一维标量,向量表示如下：
$$
D=\begin{pmatrix}
 x_{1} & y_{1}\\
  \vdots & \vdots\\
  x_{i}& y_{i}\\
 \vdots & \vdots\\
  x_{n}&y_{n}
\end{pmatrix},
X=\begin{pmatrix}
 x_{1} \\
  \vdots \\
  x_{i}\\
 \vdots \\
  x_{n}
\end{pmatrix},
Y=\begin{pmatrix}
y_{1}\\
 \vdots\\
 y_{i}\\
 \vdots\\
 y_{n}
\end{pmatrix}
$$

### 2.4.3**一元线性回归模型：**


$$
\hat{y}_{i}=f(x_{i})=wx_{i}+b\tag{2.01}\\
$$

### 2.4.4 **损失函数：**

$$
\begin{align}
&loss_{i}=e_{i} =\hat{y}_{i}-y_{i}\tag{2.02}\\
\\
&Cost(SSE)=J(w,b)= \sum_{i=1}^{n}(\hat{y}_{i}-y_{i})^{2}=\sum_{i=1}^{n}[(wx_{i}+b)-y_{i}]^{2}\tag{2.03.1}\\
\\
&J(w,b)= \sum_{i=1}^{n}(\hat{y}_{i}-y_{i})^{2}=\sum_{i=1}^{n}[(wx_{i}+b)-y_{i}]^{2}\tag{2.03.2}\\
\\

\end{align}
$$

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/v2-097e98e86ac6319273597862a0df413d_720w.jpg" width =500>

### 2.4.5 求解参数$(w,b)$的解析解

1. 从损失函数（代价函数）$J(w,b)$可知，显然这是一个凸函数，具有全局最优解。
   $$
   (w^{*},b^{*})=\underset{w,b}{arg min}J(w,b)=\underset{w,b}{arg min} \sum_{i=1}^{n}(y_{i}-wx_{i}-b)^{2}\tag{2.04}
   $$

   

2. $J(w,b)$分别对$w$和$b$求偏导,然后分别令倒数为0，便可就得$w$和$b$的最优解。
   $$
   \begin{align}
   \\
   &\left\{\begin{matrix}
    \frac{\partial J(w,b)}{\partial w}=2\sum_{i=1}^{n}(wx_{i}+b-y_{i})x_{i} \\ 
    \frac{\partial J(w,b)}{\partial b}=2\sum_{i=1}^{n}[(wx_{i}+b)-y_{i}]
   \end{matrix}\right.\tag{2.05}\\
   \\
   &\left\{\begin{matrix}
    \frac{\partial J(w,b)}{\partial w}=\sum_{i=1}^{n}(wx_{i}+b-y_{i})x_{i}=0 \\ 
    \frac{\partial J(w,b)}{\partial b}=\sum_{i=1}^{n}[(wx_{i}+b)-y_{i}]=0
   \end{matrix}\right.\tag{2.06}\\
   \end{align}
   $$

3. **$(w^{*},b^{*})$求解结果：**
   $$
   \begin{align}
   &w=\frac{\sum _{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}\tag{2.07} \\
   \\
   &b=\bar{y}-w\bar{x}\tag{2.08}
   \end{align}
   $$

4. $(w^{*},b^{*})$**求解过程 ：**

   **分析：**显然$J（w,b）$对$b$求导更简单，对$w$更复杂，所以先对$b$求导，在对$w$求导；由公式==（2.06）==可知两个未知数两个方程，可用**高斯消元法**求解。
   $$
   \begin{align}
   &1.求解b：\\
   \\
   &\frac{\partial J(w,b)}{\partial b}=2\sum_{i=1}^{n}[(wx_{i}+b)-y_{i}]=0\tag{2.09}\\
   &=>\sum_{i=1}^{n}[y_{i}-(wx_{i}+b)]=0\\
   &=>\sum_{i=1}^{n}y_{i}-w\sum_{i=1}^{n}x_{i}-\sum_{i=1}^{n}b=0\\
   &=>b=\frac{1}{n}\sum_{i=1}^{n}y^{i}-\frac{1}{n}w\sum_{i=1}^{n}x_{i}\tag{2.10}\\
   \\
   &=>b=\bar{y}-w\bar{x}\tag{2.11}\\
   \\
   &2.求解w：\\
   \\
   &\frac{\partial J(w,b)}{\partial w}=2\sum_{i=1}^{n}(wx_{i}+b-y_{i})x_{i}\tag{2.12}\\
   &\frac{\partial J(w,b)}{\partial w}=2\sum_{i=1}^{n}(wx_{i}+b-y_{i})x_{i}=0\tag{2.13}\\
   &=>\sum_{i=1}^{n}(y_{i}-wx_{i}-b)x_{i}=0\\
   &=>\sum_{i=1}^{n}(y_{i}x_{i}-wx_{i}^2-bx_{i})=0\tag{2.14}\\
   &=>w\sum_{i=1}^{n}x_{i}^2=\sum_{i=1}^{n}y_{i}x_{i}-\sum_{i=1}^{n}bx_{i}\\
   \\
   &=>w = \frac{\sum_{i=1}^{n}y_{i}x_{i}-\sum_{i=1}^{n}bx_{i}}{\sum_{i=1}^{n}x_{i}^2}\tag{2.15}\\
   \\
   &3.高斯消元法：\\
   \\
   &\left\{\begin{matrix} 
   2.11\\
   2.14
   \end{matrix}\right. =
   \left\{\begin{matrix} 
   b=\bar{y}-w\bar{x}\\
   \sum_{i=1}^{n}(y_{i}x_{i}-wx_{i}^2-bx_{i})=0
   \end{matrix}\right. \tag{2.16}\\
   
   &=>\sum_{i=1}^{n}(y_{i}x_{i}-wx_{i}^{2}-(\bar{y}-w\bar{x})x_{i})=0\\
   &=>\sum_{i=1}^{n}(y_{i}x_{i}-\bar{y}x_{i})-\sum_{i=1}^{n}(wx_{i}^{2}-w\bar{x}x_{i})=0\\
   &=>w\sum_{i=1}^{n}(x_{i}^{2}-\bar{x}x_{i})=\sum_{i=1}^{n}(y_{i}x_{i}-\bar{y}x_{i})\\
   \\
   &=>w=\frac{\sum_{i=1}^{n}(y_{i}x_{i}-\bar{y}x_{i})}{\sum_{i=1}^{n}(x_{i}^{2}-\bar{x}x_{i})}\tag{2.17} \quad \quad 正常人做到这里就可以了 \\
   \\
   &=>w=\frac{\sum_{i=1}^{n}y_{i}x_{i}-\bar{y}\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}^{2}-\bar{x}\sum_{i=1}^{n}x_{i}}\tag{2.18} \\
   \\
   &\bar{y}\sum_{i=1}^{n}x_{i}=\frac{1}{n}\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i} =\bar{x}\sum_{i=1}^{n}y_{i}\tag{2.19}\\
   \\
   &\bar{x}\sum_{i=1}^{n}x_{i}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\sum_{i=1}^{n}x_{i}=\frac{1}{n}(\sum_{i=1}^{n}x_{i})^{2}\tag{2.20} \\
   \\
   &=>w=\frac{\sum_{i=1}^{n}(y_{i}x_{i}-\bar{y}x_{i})}{\sum_{i=1}^{n}(x_{i}^{2}-\bar{x}x_{i})}\tag{2.21} \\
   \\
   &=>w=\frac{\sum_{i=1}^{n}y_{i}x_{i}-\bar{y}\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}(x_{i}^{2}-\bar{x}x_{i})}\tag{2.22} \\
   \\
   &\sum_{i=1}^{n}\bar{y}x_{i}=\bar{y}\sum_{i=1}^{n}x_{i}=\frac{1}{n}\sum_{i=1}^{n}y_{i}\sum_{i=1}^{n}x_{i} =\bar{x}\sum_{i=1}^{n}y_{i}=n\bar{x}\bar{y}=\sum_{i=1}^{n}\bar{x}\bar{y}\tag{2.23}\\
   \\
   &\sum_{i=1}^{n}\bar{x}x_{i}=\bar{x}\sum_{i=1}^{n}x_{i}=\bar{x}.n.(\frac{1}{n} \sum_{i=1}^{n}x_{i})=\bar{x}.n.\bar{x}=n\bar{x}^2=\sum_{i=1}^{n}\bar{x}^2 \tag{2.24} \\
   \\
   &=>w=\frac{\sum_{i=1}^{n}(y_{i}x_{i}-y_{i}\bar{x}-x_{i}\bar{y}+\bar{x}\bar{y})}{\sum_{i=1}^{n}(x_{i}^{2}-x_{i}\bar{x}-x_{i}\bar{x}+\bar{x}^{2}  )} \tag{2.25}\\
   \\
   &=>w=\frac{\sum _{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2} \tag{2.26} \quad \quad  数学上的完美统一\\
   \\
   &4.向量化表示：
   \\
   \\
   
   &x=\begin{pmatrix}
   x_{1}\quad x_{2} \quad \dots \quad x_{n}
   \end{pmatrix}\\
   &x_d=\begin{pmatrix}
   x_{1}-\bar{x}\quad x_{2}-\bar{x} \quad \dots \quad x_{n}-\bar{x}
   \end{pmatrix}\\
   &y=\begin{pmatrix}
   y_{1}\quad y_{2} \quad \dots \quad y_{n}
   \end{pmatrix}\\
   &y_d=\begin{pmatrix}
   y_{1}-\bar{y}\quad y_{2}-\bar{y} \quad \dots \quad y_{n}-\bar{y}
   \end{pmatrix}\\
   &x_{d}y_{d}^{T}=
   \begin{pmatrix}
   x_{1}-\bar{x}\quad x_{2}-\bar{x} \quad \dots \quad x_{n}-\bar{x}
   \end{pmatrix}\times 
   \begin{pmatrix}
   y_{1}-\bar{y}\\y_{2}-\bar{y}\\ \vdots  \\ y_{n}-\bar{y}
   \end{pmatrix}\tag{2.27}
   \\
   \\
   &x_{d}x_{d}^{T}=\begin{pmatrix}
   x_{1}-\bar{x}\quad x_{2}-\bar{x} \quad \dots \quad x_{n}-\bar{x}
   \end{pmatrix}\times 
   \begin{pmatrix}
   x_{1}-\bar{x}\\x_{2}-\bar{x}\\ \vdots  \\ x_{n}-\bar{x}
   \end{pmatrix}\tag{2.28}
   \\
   &=>w=\frac{x_{d}y_{d}^{T}}{x_{d}x_{d}^{T}} \tag{2.29}\quad \quad 程序实现的完美表达式\\
   \\
   &=>b=\bar{y}-w\bar{x}\tag{2.30}\\
   \\
   \end{align}
   $$

# 3. 多元线性回归

## 3.1 多元线性回归引入

在生活中遇到的问题通常都不是**一元线性回归问题**，因为一般影响一件事物的因素有很多个原因，所以生活中更常见的是**多元问题**。

比如还拿那个**房价预测**举例，明显**房价**不可能只由**房屋面积**决定，肯定还要考虑很多其他属性，如房间数量、楼间距、离学校距离等等等等

所以现在依旧是**$n$个样本**做预测，但每个样本不止一个属性，而是由**$d$个属性**来描述。因此，我们引入**多元线性回归**。

## 3.2 多元线性回归模型

### 3.2.1 多元线性回归模型数据集

对于给定的数据集$D={(x_{1},y_{1},(x_{2},y_{2}),\dots,(x_{n},y_{m}))}\quad$,其中， $x_{i}=(x_{i}^{1};x_{i}^{2};\dots;x_{i}^{d})$,$\quad y_{i}\in \mathbb{R}\quad$，“**线性回归**”试图学得一个**线性模型**，尽可能准确的预测未统计过得的同类数据的的标记值。
$$
\begin{align}
\\
&D=\{{(x_{1},y_{1}),(x_{2},y_{2})},\dots,(x_{n},y_{n})\}\tag{3.01}\\
\\
&x_{i}=\begin{pmatrix}
 x_{i}^{1}\\
 x_{i}^{2}\\
 \vdots \\
 x_{i}^{d}
\end{pmatrix},\quad
w = \begin{pmatrix}
w^{1}\\
w^{2} \\
\vdots  \\
w^{d}
\end{pmatrix},\quad
y = \begin{pmatrix}
y_{1}\\
y_{2} \\
\vdots  \\
y_{d}
\end{pmatrix}\tag{3.02}\\
\\
&w^{T}=\begin{pmatrix}
w^{1}\quad w^{2} \quad \dots \quad w^{d}
\end{pmatrix},
X^{d\times n}=\begin{pmatrix}
  x_{1}^{1}& x_{2}^{1} & x_{i}^{1} & x_{n}^{1}\\
  x_{1}^{2}& x_{2}^{2} & x_{i}^{2} & x_{n}^{2}\\
  \vdots &  \vdots & \ddots & \vdots  \\
  x_{1}^{d}& x_{2}^{d} & x_{i}^{d} & x_{n}^{d}
\end{pmatrix}_{d\times n}\tag{3.03}\\
\\
\end{align}
$$

### 3.2.2 多元线性回归模型



仿照一元线性回归公式![[公式]](https://www.zhihu.com/equation?tex=f(x_i)%3Dwx_i%2Bb)，我们可以得到**多元线性回归公式**：
$$
\begin{align}
\\
&f(x_{i}) = w^{1}x_{i}^{1}+w^{2}x_{i}^{2}+···+w^{d}x_{i}^{d}+b\tag{3.04} \\
\\
&f(x_{i}) = w^{T}x_{i}+b\tag{3.05}\\
\\
&其中,x_{i}^{d}表示第i个样本的第d个特征,w^{d}表示样本第d个特征的权重
\\
\end{align}
$$
与一元线性回归类似，可以利用**最小二乘法**来对$w$和$b$进行估计。为了便于讨论我们吧$w$和$b$吸收入向量形式$\hat{w}=(w;b),\quad$相应的，把数据集$D$表示为一个$n \times (d+1)$大小的矩阵$X,\quad$其中每一行代表一个条记录（也就是是一个样本的$d$个特征），每行的前$d$个元素对应于样本的$d$个特征，最后一个元素恒为$1$(为了与偏置$b$对应起来)。

$X$：把数据集表示为一个 $n\times (d+1)$大小的矩阵 $X$，其中每行对应一个样本，该行前$d$个元素对应于样本的$d$个属性值，最后一个元素恒置为$1$,如公式$(3.06):$
$$
\begin{align}
\\
&X^{n\times (d+1)}=\begin{pmatrix}
  x_{1}^{1}& x_{1}^{2} & \dots & x_{1}^{d}&1\\
  x_{2}^{1}& x_{2}^{2} & \dots & x_{2}^{d}&1\\
  \vdots &  \vdots & \ddots & \vdots  &\vdots \\
  x_{n}^{1}& x_{n}^{2} & \dots  & x_{n}^{d}&1
\end{pmatrix}_{n\times (d+1)}
=\begin{pmatrix}
  x_{1}^{T}& 1\\
  x_{2}^{T}& 1\\  
  \vdots &\vdots  \\
  x_{n}^{T}& 1  
\end{pmatrix}_{n \times(d+1)}\tag{3.06}\\
\end{align}\\
\\
$$

**$\hat{w}:\quad$把$w$和$b$吸入向量的形式：**
$$
\begin{align}
&\hat{w}=\begin{pmatrix}
w\\
b
\end{pmatrix}=\begin{pmatrix}
 w^{1}\\
 w^{2}\\
 \vdots \\
 w^{d}\\
b
\end{pmatrix}_{(d+1) \times 1}\tag{3.07}\\
\end{align}\\
$$
$y\quad$**还是原来的$y:$**
$$
\begin{align}
&y=\begin{pmatrix}
 y_{1}\\
 y_{2}\\
 \vdots\\
 y_{n}
\end{pmatrix}\tag{3.08}\\
\end{align}
$$
**多元线性回归模型**$f(X)$：
$$
f(X)=X\hat{w}=\begin{pmatrix}
  x_{1}^{1}& x_{1}^{2} & \dots & x_{1}^{d}&1\\
  x_{2}^{1}& x_{2}^{2} & \dots & x_{2}^{d}&1\\
  \vdots &  \vdots & \ddots & \vdots  &\vdots \\
  x_{n}^{1}& x_{n}^{2} & \dots  & x_{n}^{d}&1
\end{pmatrix}
\begin{pmatrix}
 w^{1}\\
 w^{2}\\
 \vdots \\
 w^{d}\\
b
\end{pmatrix}\tag{3.09}\\
$$


### 3.2.3 多元线性回归模型损失函数

**根据已知标记$y$与$f(X)$作差可得：**
$$
\begin{align}
&y-X\hat{w}=
\begin{pmatrix}
 y_{1}\\
 y_{2}\\
 \vdots\\
 y_{n}
\end{pmatrix}-
\begin{pmatrix}
  x_{1}^{1}& x_{1}^{2} & \dots & x_{1}^{d}&1\\
  x_{2}^{1}& x_{2}^{2} & \dots & x_{2}^{d}&1\\
  \vdots &  \vdots & \ddots & \vdots  &\vdots \\
  x_{n}^{1}& x_{n}^{2} & \dots  & x_{n}^{d}&1
\end{pmatrix}
\begin{pmatrix}
 w^{1}\\
 w^{2}\\
 \vdots \\
 w^{d}\\
b
\end{pmatrix}\tag{3.10}\\
\\
&y-X\hat{w}=\begin{pmatrix}
 y_{1}-(w^{1}x_{1}^{1}+w^{2}x_{1}^{2}+\dots +w^{d}x_{1}^{d}+b)\\
 y_{2}-(w^{1}x_{2}^{1}+w^{2}x_{2}^{2}+\dots +w^{d}x_{2}^{d}+b)\\
 \vdots \\
 y_{n}-(w^{1}x_{n}^{1}+w^{2}x_{n}^{2}+\dots +w^{d}x_{n}^{d}+b)\\
\end{pmatrix}\tag{3.11}\\
\\
&y-X\hat{w}=\begin{pmatrix}
 y_{1}-(\sum_{i=1}^{d}w^{i}x_{1}^{i}+b)\\
 y_{2}-(\sum_{i=1}^{d}w^{i}x_{2}^{i}+b)\\
 \vdots \\
 y_{n}-(\sum_{i=1}^{d}w^{i}x_{n}^{i}+b)\\
\end{pmatrix}_{n \times 1}\tag{3.12}\\
\\
&y-X\hat{w}=\begin{pmatrix}
 y_{1}-f(x_{1}) \\
 y_{2}-f(x_{2}) \\
 \vdots \\
 y_{n}-f(x_{n}) \\
\end{pmatrix}_{n \times 1}\tag{3.13}\\
\\
&y-X\hat{w}=\begin{pmatrix}
 y_{1}-x_{1}^{T}\hat{w}  \\
 y_{2}-x_{2}^{T}\hat{w}  \\
 \vdots \\
 y_{n}-x_{n}^{T}\hat{w}  \\
\end{pmatrix}_{n \times 1}\tag{3.14}\\
\\
\end{align}
$$
**然后根据 $\sum_{i=1}^{n} \varphi_{i}^{2}=\Phi^{T}\Phi $ , 综上可得多元线性回归损失函数$J(\hat{w})$的矩阵形式如下：**
$$
\begin{align}\\
&Cost(SSE)=J(\hat{w})=\sum_{i=1}^{n}(y_{i}-f(x_{i}))^{2}=(y-X\hat{w})^{T}(y-X\hat{w})\tag{3.15}\\
\\
&J(\hat{w})=(y-X\hat{w})^{T}(y-X\hat{w})\tag{3.16}\\
\\
&J(\hat{w})=y^{T}y-y^{T}X\hat{w}-\hat{w}^{T}X^{X}y+\hat{w}^{T}X^{T}X\hat{w} \tag{3.17}\\
\\
\end{align}
\\
$$

###  2.3.4 多元线性回归最小二乘法

**求解参数$(\hat{w}):$**
$$
\begin{align}
\\
&(\hat{w}^{*} )=\underset{\hat{w}}{argmin} (y-X\hat{w})^{T}(y-X\hat{w})\tag{3.18}\\
\\
&(\hat{w}^{*})=\underset{\hat{w}}{argmin}y^{T}y-y^{T}X\hat{w}-\hat{w}^{T}X^{X}y+\hat{w}^{T}X^{T}X\hat{w} \tag{3.19}\\
\\
\end{align}
$$
**损失函数$J(\hat{w})$对$\hat{w}$求偏导$\frac{\partial J(\hat{w})}{\partial \hat{w}} :$**
$$
\begin{align}
\\
&\frac{\partial J(\hat{w})}{\partial \hat{w}} = \frac{\mathrm {d} y^{T}y}{\mathrm{d} \hat{w} } -\frac{\partial y^{T}X\hat{w}}{\partial \hat{w}} -\frac{\partial \hat{w}^{T}X^{T}y}{\partial \hat{w}} +\frac{\partial \hat{w}^{T}X^{T}X\hat{w}}{\partial \hat{w}} \tag{3.20} \\ 
\\
&\frac{\partial J(\hat{w})}{\partial \hat{w}} = 0-X^{T}y-X^{T}y+(X^{T}X+X^{T}X)\hat{w} \tag{2.21}\\
\\
&\frac{\partial J(\hat{w})}{\partial \hat{w}} =2X^{T}(X\hat{w}-y)\tag{2.22}\\
\\
\\
\end{align}
$$
**矩阵求导公式：**
$$
\begin{align}
\\
&\frac{\mathrm{d} a^{T}x}{\mathrm{d} x} =\frac{\partial x^{T}a}{\partial x} =a,\quad \frac{\partial x^{T}Ax}{\partial x}=(A+A^{T})\hat{w}  \tag{3.23}\\
\\
\end{align}
$$
**矩阵求导参考书：**

1. Matrix Cook Book <http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf>

2. 矩阵分析与应用 张贤达 清华大学出版社



这次的解方程并没有像一元线性回归那么简单，我们需要分情况讨论：

**第一种情况当$X^{T}X$为满秩矩阵或正定矩阵时：**

**令损失函数$J(\hat{w})$对$\hat{w}$求偏导$\frac{\partial J(\hat{w})}{\partial \hat{w}}=0 $可解的$\hat{w}$的解析最优解****
$$
\begin{align}
\\
&\frac{\partial J(\hat{w})}{\partial \hat{w}} =2X^{T}(X\hat{w}-y)=0\tag{2.24}\\
\\
\\
&=>\hat{w}=(X^{T}X)^{-1}X^{T}y\tag{3.25}\\
\\
\end{align}
$$
其中$(X^{T}X)^{-1}$是$(X^{T}X)$的逆矩阵。

**最终学得的最合适的多元线性回归模型为公式$(3.27)$:**
$$
\begin{align}
&\widehat{x_{i}}^{T}=
\begin{pmatrix}
 x_{i}\\
1
\end{pmatrix}\tag{3.26}\\
\\
&f(\widehat{x_{i}}^{T})=\widehat{x_{i}}^{T}(X^{T}X)^{-1}X^{T}y\tag{3.27}
\\
\end{align}
$$


**第二种情况：当** ![[公式]](https://www.zhihu.com/equation?tex=X%5ETX) **不为满秩矩阵时**

在实际中可能会出现是奇异矩阵，往往是因为特征值之间**不独立**。这时候需要对特征值进行筛选，剔除那些存在**线性关系的特征值**（好比在预测房价中，特征值1代表以英尺为尺寸计算房子，特征值2代表以平方米为尺寸计算房子，这时特征值1和特征值2只需要留1个即可）

也就是现实任务中我们会遇到大量变量（对应大量待算属性，大量未知的$w$），其数目甚至超过了样本数目，导致 ![[公式]](https://www.zhihu.com/equation?tex=X) 的列数多于行数，![[公式]](https://www.zhihu.com/equation?tex=X%5ETX) 显然不满秩。

此时可以解出**多个** ![[公式]](https://www.zhihu.com/equation?tex=%5Cwidehat%7Bw%7D) ，它们都能使均方误差最小化。到底选择哪一个解作为输出呢？将由学习算法的归纳偏好决定，常见的做法是引入**正则化项**。但是编程的时候通常不采用这种方法，多数时候都采用**梯度下降法**求解。

# 4. 梯度下降法

## 4.1 导数

### 4.1.1 导数的定义

<center><img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210607101512.png"></center>

设[函数](https://baike.baidu.com/item/函数)$y=f（x）$在点$x_{0}$的某个[邻域](https://baike.baidu.com/item/邻域)内有定义，当[自变量](https://baike.baidu.com/item/自变量/6895256)$x$在$x_{0}$处有增量$Δx$，$x(_{0}+Δx)$也在该邻域内时，相应地函数取得[增量](https://baike.baidu.com/item/增量)$\quad Δy=f（x_{0}+Δx）-f(x0),$如果$Δy$与$Δx$之比当$Δx→0$时[极限](https://baike.baidu.com/item/极限)存在，则称函数$y=f（x）$在点$x_{0}$处可导， 即

![img](https://bkimg.cdn.bcebos.com/formula/4f27fa13b9ba9771d19e753d4a05d81f.svg)

### 4.1.2 导数几何意义

函数$y=f（x）$在$x_{0}$点的导数$f^{'}(x_{0})$的几何意义：表示函数曲线在点$P_{0}(x_{0},f(x_{0}))$处的切线的斜率（导数的几何意义是该函数曲线在这一点上的切线斜率,**表示函数增量变化最快的方向**）。

## 4.2 梯度下降

### 4.2.1 梯度下降引入

**梯度下降法是用来计算凸函数最小值的**。它的思路很简单，就是**猴子下山的问题**，想象在山顶放了一个球，一松手它就会顺着山坡最陡峭的地方滚落到谷底：

<table>
    <tr>
         <td>
            <center><img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210605101153805.png" width ="500"></center>
        </td>
	    <td>
            <center><img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(5).gif" width="500"></center>
        </td>
    </tr>
</table>

### 4.2.2 梯度下降原理

#### 4.2.2.1 问题引入

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(1).webp)

最优化问题是求解函数极值的问题，包括极大值和极小值。相信所有的读者对这个问题都不陌生，在初中时我们就学会了求解二次函数的极值（抛物线的顶点），高中时学习了幂函数，指数函数，对数函数，三角函数，反三角函数等各种类型的函数，求函数极值的题更是频频出现。这些方法都采用了各种各样的技巧，没有一个统一的方案。

真正的飞跃发生在大学时，微积分为我们求函数的极值提供了一个统一的思路：找函数的导数等于0的点，因为在极值点处，导数必定为0。这样，只要函数的可导的，我们就可以用这个万能的方法解决问题，幸运的是，在实际应用中我们遇到的函数基本上都是可导的。

在机器学习之类的实际应用中，我们一般将最优化问题统一表述为求解函数的极小值问题，即：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(2).webp" height=40>

其中$x$称为优化变量，f称为目标函数。极大值问题可以转换成极小值问题来求解，只需要将目标函数加上负号即可：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(3).webp" height=40>

一个优化问题的全局极小值![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGC1mZ2fj8z91mOD1wUZOScLtMgsOic48Dmmfa4ib5Gp6ZrMoPnL7pVZmBA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是指对于可行域里所有的$x$，有：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(4).webp" height=45>

在这里，我们的目标是找到全局极小值。不幸的是，有些函数可能有多个局部极小值点，因此即使找到了导数等于0的所有点，还需要比较这些点处的函数值。

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(7).gif)

#### 4.2.2.2 梯度

由于实际应用中一般都是多元函数，前面的**导数就是一元函数的梯度**，这里就直接介绍**多元函数**的情况。梯度是导数对多元函数的推广，它是多元函数对各个自变量偏导数形成的向量。多元函数的梯度定义为：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(5).webp" height = 80>



**其中$\bigtriangledown$称为梯度算子**，它作用于一个多元函数，得到一个向量。下面是计算函数梯度的一个例子：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(6).webp" height = 50>

可导函数在某一点处取得极值的必要条件是梯度为$0$，**梯度为$0$的点**称为函数的**驻点**，这是疑似极值点。需要注意的是，**梯度为$0$只是函数取极值的必要条件而不是充分条件，即梯度为$0$的点可能不是极值点。**

在这里我们可能会问：直接求函数的导数/梯度，然后令导数/梯度为0，解方程，问题不就解决了吗？事实上没这么简单，因为这个方程可能很难解。比如下面的函数：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(7).webp" height=50>

我们分别对$x$和$y$求偏导数，并令它们为$0$，得到下面的方程组：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(8).webp" height="100">

这个方程非常难以求解，对于有指数函数，对数函数，三角函数的方程，我们称为超越方程，求解的难度并不比求极值本身小。

精确的求解不太可能，因此只能求近似解，这称为数值计算。工程上实现时通常采用的是迭代法，它从一个初始点$x_{0}$开始，反复使用某种规则从移$x_{k}$动到下一个点$x_{k+1}$，构造这样一个数列，直到收敛到梯度为$0$的点处。即有下面的极限成立：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(9).webp" height = 50>

这些规则一般会利用一阶导数信息即**梯度**；或者二阶导数信息即Hessian矩阵。这样迭代法的核心是得到这样的由上一个点确定下一个点的迭代公式：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(10).webp" height=50>

这个过程就像我们处于山上的某一位置，要到山底找水喝，因此我们必须到达最低点处：

![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGC6pgfzSwUAVLcUaqFbrPcmEnJ4dhN0yxmRvajia54cIav2GpEP0SAdFg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

此时我们没有全局信息，根本就不知道哪里是地势最低的点，只能想办法往山下走，走 一步看一步。刚开始我们在山上的某一点处，每一步，我们都往地势更低的点走，以期望能走到山底。

#### 4.2.2.3 梯度推导过程

首先我们来看一元函数的泰勒展开，以便于更好的理解多元函数的泰勒展开。如果一个一元函数$n$阶可导，它的泰勒展开公式为：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(11).webp">



如果在某一点处导数值大于$0$即导数的方向为正$(+)$，则函数在此处是增函数，加大$x$的值函数值会增加，减小$x$的值$(-)$函数会减小。相反的，如果在某一点处导数值小于$0（-）$，则函数是减函数，增加$x$的值函数值会减小$(+)$，减小$x$的值函数会增加。因此我们可以得出一个结论：**如果$x$的变化很小，并且变化值与导数值反号，则函数值下降**。**对于一元函数，$x$的变化只有两个方向，要么朝左，要么朝右。**

 

下面我们把这一结论推广到多元函数的情况。**多元函数$f(x)$在$X$点处的泰勒展开为**：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(12).webp" height=50>

这里我们忽略了二次及更高的项。其中，**一次项是梯度向量**$\bigtriangledown f(X)$与**自变量$\bigtriangleup X$增量**的内积$(\bigtriangledown f(X))^{T}\bigtriangleup X$，这等价于一元函数的${f}'(x_{0})\bigtriangleup x$。这样，函数的增量与自变量的增量$\bigtriangleup X$、函数梯度的关系可以表示为：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(13).webp" height = 50>

如果$\bigtriangleup X$足够小，在$X$的某一邻域内，则我们可以**忽略二次及以上的项**，有：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(14).webp" height =50>

这里的情况比一元函数复杂多了，![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCYSFCcvSJ7Eunpba4OMJ8BV2JuHvFTxxmLDt5O80ic0jd4o5StGaNW0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是一个向量，![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCYSFCcvSJ7Eunpba4OMJ8BV2JuHvFTxxmLDt5O80ic0jd4o5StGaNW0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)有无穷多种方向，该往哪个方向走呢？如果能保证：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(211).webp" height=50>

则有：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605184836.webp" height="50">

即函数值递减，这就是下山的正确方向。因为有：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605185015.webp" height="60">

在这里，||·||表示向量的模，![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCoQ4v812vQt2aJfI7yONbXj7VwiavJDkiaibBQKlemFo8ttjiaP20XNSqcg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)是向量$\bigtriangledown f(X)$和![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCYSFCcvSJ7Eunpba4OMJ8BV2JuHvFTxxmLDt5O80ic0jd4o5StGaNW0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的夹角。因为向量的模一定大于等于$0$，如果：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605185223.webp" height=40>

则能保证：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605185406.webp" height=50>

即选择合适的增量![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCYSFCcvSJ7Eunpba4OMJ8BV2JuHvFTxxmLDt5O80ic0jd4o5StGaNW0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)，就能保证函数值下降，要达到这一目的，只要保证梯度和![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCYSFCcvSJ7Eunpba4OMJ8BV2JuHvFTxxmLDt5O80ic0jd4o5StGaNW0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的夹角的余弦值小于等于0就可以了。由于有：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605185543.webp" height=40>

只有当：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605185656.webp" height=40>

时![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCCv6WPZUZX0gURRkCWwmkupBlSn3x9iaOiblSyrQMneoTKDPWsdw70QMg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)有极小值-1，此时梯度和![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCYSFCcvSJ7Eunpba4OMJ8BV2JuHvFTxxmLDt5O80ic0jd4o5StGaNW0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)反向，即夹角为180度。因此当向量![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCYSFCcvSJ7Eunpba4OMJ8BV2JuHvFTxxmLDt5O80ic0jd4o5StGaNW0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)的模大小一定时，当：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605185824.webp" height=50>

即在梯度相反的方向函数值下降的最快。此时有：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605185929.webp" height=40>

函数的下降值为：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605190029.webp" height=50>

只要梯度不为0，往梯度的反方向走函数值一定是下降的。直接用<img src = "https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605190202.webp" height=20>可能会有问题，因为x+![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCYSFCcvSJ7Eunpba4OMJ8BV2JuHvFTxxmLDt5O80ic0jd4o5StGaNW0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)可能会超出$x$的邻域范围之外，此时是不能忽略泰勒展开中的二次及以上的项的，因此步伐不能太大。一般设：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605190405.webp" height=50>

其中![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCyKjUjjic3jPaCF2cwDybBVcXYOrkGWU3nRGxUD1FOfgzjG957WxZHdg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)为一个接近于0的正数，称为步长，由人工设定，用于保证x+![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCYSFCcvSJ7Eunpba4OMJ8BV2JuHvFTxxmLDt5O80ic0jd4o5StGaNW0g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)在x的邻域内，从而可以忽略泰勒展开中二次及更高的项，则有：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605190526.webp" height=50>

从初始点![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCb5P9YEgNmGw3AvK2tblA9rwfCdiaNQrLd1SVJkjhwMIa0O5UwFQlrMA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)开始，使用如下迭代公式：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605190631.webp" height=50>

只要没有到达梯度为0的点，则函数值会沿着序列![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCrkiaPlH7nCwJZCMmBDBwvC7wZchtdPXRs5F8KwRbYaGwGuCXHOkyNkQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)递减，最终会收敛到梯度为0的点，这就是梯度下降法。迭代终止的条件是函数的梯度值为0（实际实现时是接近于0)，此时认为已经达到极值点。注意我们找到的是梯度为0的点，这不一定就是极值点，后面会说明。梯度下降法只需要计算函数在某些点处的梯度，实现简单，计算量小。

#### 4.2.2.4 梯度下降实现

下面我们介绍梯度下降法实现时的一些细节问题，包括初始值的设定，学习率的设定，下面分别进行介绍。

##### 初始值的设定

一般的，对于不带约束条件的优化问题，我们可以将初始值设置为0，或者设置为随机数，对于神经网络的训练，一般设置为随机数，这对算法的收敛至关重要。

##### 学习率的设定

学习率设置为多少，也是实现时需要考虑的问题。最简单的，我们可以将学习率设置为一个很小的正数，如0.001。另外，可以采用更复杂的策略，在迭代的过程中动态的调整学习率的值。比如前1万次迭代为0.001，接下来1万次迭代时设置为0.0001。

##### 面临的问题

在实现时，梯度下降法可能会遇到一些问题，典型的是局部极小值和鞍点问题，下面分别进行介绍。

##### 局部极小值

有些函数可能有多个局部极小值点，下面是一个例子：

![图片](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmbZyH56TS9jKRokzZHIsGCsZT3TbZUxkEicddGeU5Mt01M774TjVKicwliaHQt7JHkSI2NWd4kQtWNg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这张图中的函数有3个局部极值点，分别是A，B和C，但只有A才是全局极小值，梯度下降法可能迭代到B或者C点处就终止。



##### 鞍点问题

鞍点是指梯度为0，Hessian矩阵既不是正定也不是负定，即不定的点。下面是鞍点的一个例子，假设有函数：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605191020.webp" height=45>

显然在(0, 0)这点处不是极值点，但梯度为0，下面是梯度下降法的运行结果：

<table>
    <tr>
    	<td>
            <img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605191251.webp">
        </td>
        <td>
            <img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605191323.gif">
        </td>
    </tr>
    <tr>
    	<td>
            <center>图1</center>
        </td>
        <td>
            <center>图2</center>
        </td>
    </tr>
</table>

在这里，**图1**梯度下降法遇到了鞍点，认为已经找到了极值点，从而终止迭代过程，而这根本不是极值点。**图2**通过对梯度下降法的改进，避免掉鞍点。

#### 4.2.2.5 梯度下降法和相关延伸

当前使用的许多优化算法，是对梯度下降法的衍生和优化。在微积分中，对**多元函数的参**$\theta$数求**偏导数**，把求得的各个参数的**导数以向量的形式写出来就是梯度**。**梯度就是函数变化最快的地方**。梯度下降是迭代法的一种，在求解机器学习算法的模型参数$\theta$时，即无约束问题时，梯度下降是最常采用的方法之一。

##### 梯度下降法算法步骤

这里定义一个通用的思路框架，方便我们后面理解各算法之间的关系和改进。首先定义待优化参数$\theta$，目标函数$J(\theta)$ ，学习率为 $\eta$，然后我们进行迭代优化，假设当前的epoch为 $t$，则有：

1. 随机初始化待优化参数：$\theta=\theta_{t=0}$

2. 计算目标函数关于$t$时刻当前参数的梯度： $\bigtriangledown_{\theta_{t}}=\frac{\partial J(\theta_{t})}{\partial \theta_{t}}$

3. 根据学习率$\eta$计算当前$t$时刻的下降梯度： $\bigtriangledown_{t}=\eta \cdot \bigtriangledown_{\theta_{t}}$

4. 根据下降梯度进行更新参数$\theta=\theta_{t+1}$：$\theta_{t+1}=\theta_{t}-\bigtriangledown_{t}$ 
5. 重复$2$到$4$直到$\bigtriangledown_{\theta} \to 0$, 或者达到迭代的次数

其中，$\theta_{t+1}$ 为下一个时刻的参数， $\theta_{t}$为当前$t$时刻参数，后面的描述我们都将结合这个框架来进行。

这里提一下一些概念：

##### Gradient Descent（GD）

假设当前样本为$(x_{1},y_{i})$ ，每当样本输入时，参数即进行更新，则我们在当前时刻需要下降的梯度就是$\bigtriangledown_{t}=\eta\cdot\bigtriangledown_{\theta_{t}}$ ，则使用梯度下降法更新参数为：

![image-20210605201009623](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605201010.png)

梯度下降算法中，模型参数的更新调整，与代价函数关于模型参数的梯度有关，即沿着梯度的方向不断减小模型参数，从而最小化代价函数。基本策略可以理解为”在有限视距内寻找最快路径下山“，因此每走一步，参考当前位置最陡的方向(即梯度)进而迈出下一步，更形象的如下图：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpMfrdg30r7nrkf5zXCm02Gtc5Mc5pG8iasvCUC8TEMI7lckwdK8Cqf633lnKvgTPnp1SXRENfTTqw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**标准的梯度下降主要有两个缺点：**

- 训练速度慢：在应用于大型数据集中，每输入一个样本都要更新一次参数，且每次迭代都要遍历所有的样本，会使得训练过程及其缓慢，需要花费很长时间才能得到收敛解。
- 容易陷入局部最优解：由于是在有限视距内寻找下山的反向，当陷入平坦的洼地，会误以为到达了山地的最低点，从而不会继续往下走。所谓的局部最优解就是鞍点，落入鞍点，梯度为0，使得模型参数不在继续更新。

##### Batch Gradient Descent（BGD）

BGD相对于标准GD进行了改进，改进的地方通过它的名字应该也能看出来，也就是不再是想标准GD一样，对每个样本输入都进行参数更新，而是针对一个批量的数据输入进行参数更新。我们假设**批量训练样本总数**为 $n$，样本为${(x_{1},y_{1}),\dots,(x_{n},y_{n})}$，则在第 对样本 上损失函数关于参数的梯度为$\bigtriangledown_{\theta}J_{i}(\theta,x_{i},y_{i})$ , 则使用BGD更新参数为：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605213155.png" height=60>

从上面的公式我们可以看到，BGD其实是在一个批量的样本数据中，求取该批量样本梯度的均值来更新参数，即每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型参数，这样就会大大加快训练速度，但是还是不够，我们接着往下看。

##### Stochastic Gradient Descent（SGD）

随机梯度下降法，不像BGD每一次参数更新，需要计算整个数据样本集的梯度，而是每次参数更新时，仅仅选取一个样本 $(x_{i},y_{i})$计算其梯度，参数更新公式为：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605213324.png" height=55>

公式看起来和上面标准GD一样，但是注意了，这里的样本是从批量中随机选取一个，而标准GD是所有的输入样本都进行计算。可以看到BGD和SGD是两个极端，SGD由于每次参数更新仅仅需要计算一个样本的梯度，训练速度很快，即使在样本量很大的情况下，可能只需要其中一部分样本就能迭代到最优解，由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大（如下图），更容易从一个局部最优跳到另一个局部最优，准确度下降。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpMfrdg30r7nrkf5zXCm02GMiaeTVucTbJibajf6GHz2twia8u3N4jUfJpicn6xg0JSDq4qRVKtP6DF1Q/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

当缓慢降低学习率时，SGD会显示与BGD相同的收敛行为，几乎一定会收敛到局部（非凸优化）或全局最小值（凸优化）。

**SGD的优点：**

- 虽然看起来SGD波动非常大，会走很多弯路，但是对梯度的要求很低（计算梯度快），而且对于引入噪声，大量的理论和实践工作证明，只要噪声不是特别大，SGD都能很好地收敛。
- 应用大型数据集时，训练速度很快。比如每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。相比于标准梯度下降法的遍历全部样本，每输入一个样本更新一次参数，要快得多。

**SGD的缺点：**

- SGD在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确（次要）。
- SGD也没能单独克服局部最优解的问题（主要）。

##### Mini-batch Gradient Descent（MBGD，也叫作SGD）

小批量梯度下降法就是结合BGD和SGD的折中，对于含有 个训练样本的数据集，每次参数更新，选择一个大小为 $m(m<n)$ 的mini-batch数据样本计算其梯度，其参数更新公式如下：

<img src="https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210605213526.png" height=60>

小批量梯度下降法即保证了训练的速度，又能保证最后收敛的准确率，目前的SGD默认是小批量梯度下降算法。常用的小批量尺寸范围在50到256之间，但可能因不同的应用而异。

**MBGD的缺点：**

- Mini-batch gradient descent 不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点）。对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点所有维度的梯度都接近于0，SGD 很容易被困在这里（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是BGD的训练集全集带入，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动）。
- SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新， 且learning rate会随着更新的次数逐渐变小。

### 4.2.3梯度下降案例

#### 4.2.3.1 无约束凸优化问题

最简单的二维凸函数是抛物线 $f(x)=x^{2}$，我们的目标是：**求取使得函数值$f(x)$最小时，参数$x$的取值**；我们都知道，很容易通过求解$f(x)$的导数$f^{‘}(x)=2x|_{x=0}$  求出最小值在 $x=0$处：

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640.png)



#### 4.2.3.2 初始化参数

假设**初始的时候**$x$在$(x_{0}=10,f(x_{0}=100))$处,也就是$(10,100)$：

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(1).png)



此时，函数$f(x)$在$x_{0}=10$时的梯度$\bigtriangledown f(x_{0})$为：
$$
\bigtriangledown f(x_{0})=({f}'(x_{0})) = ((2x|_{x_{0}=10})=20)=(20)
$$
**$\bigtriangledown f(x)  $它指向函数值增长最快的方向，而 $-\bigtriangledown f(x)  $ 则是指向函数减小最快的方向：**

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(2).png)

将$x_{0}$当做$1$维向量$(x_{0})$，**通过与$-\bigtriangledown f(x) $(负梯度，也就是梯度的反方向)相加，可以将函数的梯度向$-\bigtriangledown f(0)$的方向移动一小段距离**，得到新的参数$（x）$(也就是自变量$x$)更新后的新的值：$(x_{1})$
$$
x_{1}=(x_{0})-\eta \bigtriangledown f(x_{0})|_{x_{0}=10}
$$
其中，$\eta$称为学习率，也就是移动步长，通过$\eta$；来控制每次更新$(x)$移动的距离，假设$\eta=0.2$,那么$(x_{1})$为：
$$
\begin{align}
&(x_{1})=(x_{0})-\eta \bigtriangledown f(x_{0})\\
\\
&=(10)-0.2\times (2x|_{x_{0}=10}=20)\\
\\
&=(10)-0.2 \times (20)\\
\\
&=(10)-(4)\\
\\
&=(6)\\
\\

\end{align}
$$
此时从起点$(x_{0}=10,f(x_{0})=100)$,也就是点$(10,100)$下降到了$(x_{1}=6,f(x_{1})=36)$,也就是$(6,36)$这个位置：

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(3).png)

#### 4.2.3.3 迭代更新参数

$x_{1}$的梯度$\bigtriangledown f(x_{1})$的梯度为：
$$
\begin{align}
\bigtriangledown f(x_{1})=({f}'(x_{1}))=(2x|_{x_{1}=6})=(12)
\end{align}
$$
继续沿着梯度的反方向走$\eta$,更新参数$x$,得到$x_{2}$:
$$
\begin{align}
&(x_{2})=(x_{1})-\eta \bigtriangledown f(x_{1})\\
\\
&=(6)-0.2 \times (12)\\
\\
&=3.6
\end{align}
$$
此时函数值下降到了更低的位置：$(x_{2}=3.6,f(x_{2})=(3.6)^{2})$, 也就是点$(3.6,(3.6)^{2})$:

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(4).png)

重复上述过程到第 10 次，小球基本上就到了最低点，即有 $x_{10}\approx 0$

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640.gif)

#### 4.2.3.4 梯度下降总结

把每一次迭代参数$x$的变化的和梯度向量的摸长$\left \| \bigtriangledown f(x_{i}) \right \| $ 的列出来，可以看到是在不断减小的：

|                         **迭代次数**                         | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8** | **9** | **10** |
| :----------------------------------------------------------: | :---: | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ------ |
|                           $x_{i}$                            |  10   | 6     | 3.6   | 2.16  | 1.295 | 0.78  | 0.465 | 0.17  | 0.1   | 0.06   |
| ![img](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/clip_image004.png) |  20   | 12    | 7.2   | 4.32  | 2.59  | 1.56  | 0.93  | 0.34  | 0.2   | 0.12   |

这也比较好理解，当梯度$\bigtriangledown f(x_{i})$最终趋向于 0 时有：

![image-20210605150808643](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210605150808643.png)

所以梯度下降法求出来的就是最小值（或者在附近）

#### 4.2.3.5 梯度下降的学习率

上面谈到了可以通过步长(学习率)$\eta$来控制每次移动的距离，下面来看看不同步长对最终结果的影响。

##### 学习率过小

如果设$\eta=0.01$就过于小了，迭代 20 次后离谷底还很远，实际上 100 次后都无法到达谷底：

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(1).gif)

##### 学习率适中

上面例子中用的 $\eta=0.2$ 是较为合适的步长，10 次就差不多找到了最小值：

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(2).gif)

##### 学习率较大

如果令 $\eta=1$，这个时候会来回震荡（下图看上去只有两个点，实际上在这两个点之间来来回回）

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(3).gif)

##### 学习率太大

继续加大步长，比如令 $\eta=1.1$，反而会越过谷底，不断上升：

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(4).gif)

##### 学习率总结

总结下，不同的步长$\eta$ ，随着迭代次数的增加，会导致被优化函数$f(x)$  的值有不同的变化：

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640.webp)

寻找合适的步长是个手艺活，在工程中可以将上图画出来，根据图像来手动调整：

-   $f(x)$往上走（红线），自然是 $\eta$过大，需要调低 
-  $f(x)$一开始下降特别急，然后就几乎没有变化（棕线），可能是 $\eta$较大，需要调低
-   $f(x)$几乎是线性变化（蓝线），可能是 $\eta$过小，需要调高

### 4.2.4 梯度下降高维案例-以三维为例

#### 4.2.4.1 三维案例引入

通过简单的抛物线，理解了梯度下降算法，下面再通过一个三维的例子来加强对梯度下降法的理解。假设函数为：
$$
\begin{align}
&f(x)=(x_{1})^{2}+2(x_{2})^{2}\\
\\
&{f}'(x_{1})=2x_{1}\\
\\
&{f}'(x_{2})=4x_{2}
\end{align}
$$
函数图像级等高线如下图所示：

![image-20210605153517516](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210605153517516.png)

下面用梯度下降法来寻找最小值。

#### 4.2.4.2 初始化参数$x_{0}$

设初试点为：$x_{0}=(-3.5,-3.5)$,此时的梯度为：

![image-20210605154054541](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210605154054541.png)

令步长$\eta=0.1$ ，那么下一个点为：

![image-20210605154142826](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210605154142826.png)

可以看到向最小值方向前进了一步：

![image-20210605154213918](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210605154213918.png)

#### 4.2.4.3 迭代

同样的方法找到下一个点： 

![image-20210605154257094](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210605154257094.png)

此时又向最小值靠近了：

![image-20210605154323427](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/image-20210605154323427.png)

如此迭代20次后，差不多找到了最小值：

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/640%20(6).gif)

### 4.2.4 梯度下降求解线性回归问题

#### 4.2.4.1 线性回归的梯度

在普通最小二乘法线性回归中我们计算出线性回归损失函数的梯度为$\bigtriangledown J(W)$：
$$
\begin{align}
&\frac{\partial J(\hat{w})}{\partial \hat{w}} =\sum_{i=1}^{n}(w_{t}^{T}x_{i}-y_{i})x_{i}
\\&=2X^{T}(X\hat{w}-y)\tag{2.22}\\
\\
&=>\bigtriangledown J(W)=\frac{1}{n}X^{T}(Xw-y)
\end{align}
$$

#### 4.2.4.2 批量梯度下降

批量梯度下降算法的每一步都是基于整个训练集计算梯度的。设学习率为，梯度下降算法的参数更新公式为：
$$
w_{t+1}=w_{t}-\eta \frac{1}{n}X^{T}(Xw_{t}^{T}-y)
$$


每次使用整批训练样本计算梯度，在训练集非常大时，批量梯度下降算法会运行得极慢。

#### 4.2.4.3 随机梯度下降

随机梯度下降则每一步只使用一个样本来计算梯度。设学习率为，随机梯度下降算法的参数更新公式为：

$$
w_{t+1}=w_{t}-\eta(w_{t}^{T}x_{i}-y_{i})x_{i}
$$

#### 4.2.4.4 小批量梯度下降

小批量梯度下降顾名思义，使用一小批样本计算梯度。设一小批样本的数量为$M$，小批量梯度下降算法的梯度计算公式为：
$$
\bigtriangledown J(w)=\frac{1}{M}\sum_{i=k}^{k+M}(w_{t}^{T}x_{i}-y_{i})x_{i}
$$
设学习率为$\eta$，小批量梯度下降算法的参数更新公式为：
$$
w_{t+1}=w_{t}-\eta \frac{1}{M}\sum_{i=k}^{k+M}(w_{t}^{T}x_{i}-y_{i})x_{i}
$$
也可以写成矩阵形式：
$$
w_{t+1}=w_{t}-\eta\frac{1}{N}X^{T}(Xw_{t}-y)
$$

# 5. 线性回归实战

## 5. 1 手撸 一元线性回归

### 5.1.1 code

```python
#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
==================================================
@Project -> File   ：study-ml -> simplelr
@IDE    ：PyCharm
@Author ：jhong.tao
@Date   ：2021/6/6 15:21
@Desc   ：
==================================================
"""

import numpy as np
import matplotlib.pyplot as plt

# 创建数据集
x = [1., 2., 3., 4., 5.]
y = [3.2, 5.2, 6.8, 9.5, 10.8]

x = np.array(x).reshape((5, 1))
y = np.array(y).reshape((5, 1))

plt.scatter(x, y)
plt.xlabel('x')
plt.ylabel('y')
plt.show()
print(x.shape)


# 定义模型
def model(w, b, x):
    return w * x + b


# 定义损失函数
def cost(w, b, x, y):
    n = len(y)
    return 1./(2.*n) * (np.square(y-w*x-b)).sum()


# 定义优化器
def optimizer(w, b, x, y, lr):
    n = len(y)
    y_hat = model(w, b, x)
    dw = (1./n) * ((y_hat-y) * x).sum()
    db = (1./n) * (y_hat-y).sum()
    w = w - lr *  dw
    b = b - lr * db
    return w, b


# 定义模型训练器
def iterater(w, b, x, y, lr, epochs):
    list_cost = []
    for epoch in range(epochs):
        w, b = optimizer(w, b, x, y, lr)
        loss = cost(w, b, x, y)
        list_cost.append(loss)
    return w, b, list_cost


# 模型训练
if __name__ == '__main__':
    # 初始化 模型参数
    w = 0.
    b = 0.
    lr = 0.01
    epochs = 100

    # 训练模型
    w, b, list_cost = iterater(w, b, x, y, lr, epochs)

    # 查看模型最终损失
    loss = cost(w, b, x, y)

    # 用模型做预测
    y_hat = model(w, b, x)

    # 打印模型
    print(w, b, loss)
    plt.scatter(x, y)
    plt.plot(x, y_hat)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.show()

    # 打印模型优化过程
    plt.plot(range(epochs), list_cost)
    plt.ylabel('cost')
    plt.xlabel('epoch')
    plt.show()

```

### 5.1.2 总结

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210608081345.png)

## 5.2 Pytorch 波士顿房价回归预测 

### 5.2.1  计算图

![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210608141351.png)

### 5.2.3 Data Set

- Data：http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html

  ![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210606215836.png)

  ![](https://gitee.com/jhongtao/mdpic/raw/master/mdimg/20210606215322.png)

  ### 6.3 Code with Pytorch

```python
#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
==================================================
@Project -> File   ：study-ml -> 02_lr2torch_boston
@IDE    ：PyCharm
@Author ：jhong.tao
@Date   ：2021/6/6 18:33
@Desc   ：http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html
==================================================
"""
import torch
import numpy
from matplotlib import pyplot as plt
from sklearn.datasets import load_boston
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split as split

# 数据加载
data = load_boston()
x = data['data']
y = data['target'].reshape(-1, 1)

# 标准化数据
mm_scale = MinMaxScaler()
x = mm_scale.fit_transform(x)

# 切分数据集
x_train, x_test, y_train, y_test = split(x, y, test_size=0.2)
x_train = torch.tensor(x_train, dtype=torch.float32, requires_grad=True)
x_test = torch.tensor(x_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32, requires_grad=True)
y_test = torch.tensor(y_test, dtype=torch.float32)

# 创建网络
model = torch.nn.Sequential(
    torch.nn.Linear(13, 1)
)

# 定义优化器和损失函数
cost = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)


# 模型训练
if __name__ == '__main__':
    list_loss =[]
    max_epoch = 300
    for epoch in range(max_epoch):
        # 前向计算
        y_pred = model(x_train)
        # 计算损失
        loss = cost(y_pred, y_train)
        list_loss.append(loss.detach().numpy())
        # 梯度清零
        optimizer.zero_grad()
        # 反向传播
        loss.backward()
        # 更新权重
        optimizer.step()

    # 打印模型参数
    list_par = list(model.named_parameters())
    print(list_par[0])
    print(list_par[1])

    # 测试
    y_hat = model(x_test)
    MSE = cost(y_test, y_hat)

    # 模型评估
    RMSE = numpy.sqrt(MSE.detach().numpy())
    print(RMSE)

    # 打印优化过程
    plt.plot(range(max_epoch), list_loss)
    plt.xlabel = 'epoch'
    plt.ylabel = 'loss'
    plt.show()


```



# 6 参考文献

*说明：阅读论文时进行相关思想、结构、优缺点，内容进行提炼和记录，论文和相关引用会标明出处，引用之处如有侵权，烦请告知删除。*

致谢：感谢参考文献中的各位作者。

[1 用人话讲明白线性回归LinearRegression](https://zhuanlan.zhihu.com/p/72513104)

[2 机器学习十大经典算法之最小二乘法](https://zhuanlan.zhihu.com/p/161377083)

[3 小白入门线性回归：原理+代码](https://zhuanlan.zhihu.com/p/90998021)

[4 线性回归与最小二乘法](https://zhuanlan.zhihu.com/p/36910496)

[5 三步教你从零掌握简单线性回归：理论到实践，一个不能少](https://zhuanlan.zhihu.com/p/53403675)

[6 线性回归详解（附Python代码）](https://zhuanlan.zhihu.com/p/361822098)

[7 还不了解梯度下降法？看完这篇就懂了！](https://mp.weixin.qq.com/s?__biz=MzUyMjI4MzE0MQ==&mid=2247484908&idx=1&sn=6dc335942ea3fa0e9418a6462fa55dd5&chksm=f9cf7406ceb8fd1046634c9a15fa3423658b906d00e0a6f6e0a772ef9fe93cdf12616ac3c59a&mpshare=1&scene=24&srcid=06047g9ce9idpTfOZ69lxEiC&sharer_sharetime=1622794629284&sharer_shareid=ccc4197eb1d994917255c7f2cacb0912#rd)

[8 梯度下降算法的工作原理](https://mp.weixin.qq.com/s?__biz=MzU2NTUwNjQ1Mw==&mid=2247493022&idx=1&sn=aa9c98842c213f571670a5b4562d4dcf&chksm=fcb81d64cbcf9472adafb812a49534155eb649258f8573c9a45fd96d0ae419e94f8faafc898f&mpshare=1&scene=24&srcid=0604aau8vrySkHF8QvOlDI76&sharer_sharetime=1622794693789&sharer_shareid=ccc4197eb1d994917255c7f2cacb0912#rd)

[9 如何理解梯度下降法？](https://mp.weixin.qq.com/s/lCL-QTitZyB7rade6NkDIA)

[10 用高中数学理解 AI “深度学习”的基本原理](https://www.cnblogs.com/pinard/p/5970503.html)

[11梯度下降算法的工作原理](https://mp.weixin.qq.com/s/DP6LJAWUKvL_f9HwXkaIiw)

[12 各种 Optimizer 梯度下降优化算法总结](https://mp.weixin.qq.com/s/BfbEplZmQEHzO3WVlDPH7Q)

[13 理解梯度下降法](https://mp.weixin.qq.com/s/lqwUkimO4irkIZmAnp0bcg)

[14 一文梳理梯度下降优化算法](https://mp.weixin.qq.com/s/Mx1yUTH9TNw_4ZsP5k-vPg)

[15 梯度下降线性回归](https://mp.weixin.qq.com/s/h9fApjr0k4civPVV1aMpBQ)

[16 普通最小二乘法线性回归](https://mp.weixin.qq.com/s?__biz=Mzg2MzM4Njk4MA==&mid=2247490005&idx=1&sn=0014e8b4b1115a906580b6f225530d5c&chksm=ce7835def90fbcc831e53f5d42eda3e6f3e2a1c26482e6e39cd88e459da87efb15286d5532fd&token=1490570571&lang=zh_CN&scene=21#wechat_redirect)

[17 如何理解梯度下降法？](https://mp.weixin.qq.com/s/SlTV6lbPnauf36bZLXglCw)

